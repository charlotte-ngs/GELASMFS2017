[
["index.html", "Angewandte Statistische Methoden in den Nutztierwissenschaften Vorwort Motivation Einordnung Lernziele", " Angewandte Statistische Methoden in den Nutztierwissenschaften Peter von Rohr 2017-04-20 Vorwort Dieses Dokument umfasst die kompletten Unterlagen zur Vorlesung Angewandte Statistische Methoden in den Nutztierwissenschaften. Der Titel dieser Vorlesung ist sehr allgemein gehalten. Dies würde es erlauben einen grosszügigen Überblick über eine breite Palette an statistischen Methoden, welche in den Nutztierwissenschaften eingesetzt werden, zu geben. Wir schlagen an dieser Stelle aber einen anderen Weg ein, und fokussieren uns auf die statistischen Methoden in der genomischen Selektion. Nur diese bewusste Wahl eines spezifischen Gebietes ermöglicht es uns, den behandelten Stoff angemessen zu vertiefen. Im anschliessenden Unterabschnitt wollen wir die hier getroffene Entscheidung der Fokusierung auf die genomische Selektion motivieren. Dabei wird klar, dass wir mit der Wahl des Themas der multiplen linearen Regression als Ausgangspunkt auch eine Leserschaft ansprechen, welche nicht primär an der Tierzucht interessiert ist. Motivation Vom Standpunkt der statistischen Modellierung, ist das einfache lineare Modell mit fixen Effektstufen für den Einsatz in der genomischen Selektion ausreichend. Diese Art von Modellen werden auch als Regressionsmodelle bezeichnet. Die Problematik entsteht erst bei der Technik, welche wir für die Schätzung der unbekannten Parameter verwenden können. In der klassischen Regressionsanalyse ist die Methode der kleinsten Quadrate (Least Squares) die Methode der Wahl. Least Squares können wir aber für die genomische Selektion nicht verwenden, da die Anzahl unbekannter Parameter (\\(p\\)) grösser ist als die Anzahl Beobachtungen (\\(n\\)). Mit der steigenden Grösse und Komplexität von aktuellen Datensätzen tritt das soeben beschriebene Problem nicht nur in der Tierzucht auf, sondern es gibt eine breite Palette von Anwendungen. In der Vorlesung beschrieben wir diese Problematik am Beispiel der genomischen Selektion und es werden alternative Techniken zur Schätzung von Parametern vorgeschlagen. Da die Methode der multiplen Regressionsanalyse in früheren Vorlesungen behandelt wurde, bietet diese ein idealer Ausgangspunkt für den in dieser Veranstaltung präsentierten Stoffinhalt. Einordnung Die Vorlesung Angewandte Statistische Methoden in den Nutztierwissenschaften ist eine halb-semestrige Veranstaltung und wird im Masterstudiengang Agrarwissenschaften der ETH Zürich angeboten. Lernziele Für die Verwendung des hier präsentierten Stoffs schlagen wir die folgenden Lernziele vor. Die Studierenden … kennen die Eigenschaften der multiplen linearen Regression und können einfache Datensätze mithilfe der Regressionsmethode analysieren wissen wieso multiple lineare Regressionen bei der genomischen Selektion nicht brauchbar ist kennen die in der genomischen Selektion verwendeten statistischen Verfahren, wie BLUP-basierte Verfahren, Bayes’sche Verfahren und die LASSO Methode können einfache Übungsbeispiele mit der Statistiksoftware R erfolgreich bearbeiten. "],
["intro.html", "Kapitel 1 Einführung 1.1 Beschreibung des Problems 1.2 Rückblick 1.3 Genomische Selektion 1.4 Zusammenfassung 1.5 Ausblick", " Kapitel 1 Einführung Den in dieser Vorlesung präsentierte Stoff kann aus mehreren Gesichtspunkten betrachtet werden. Aus Sicht der Tierzucht behandeln wir die statistischen Methoden, welche in der genomischen Selektion angewendet werden. Für Statistiker stellen wir verschiedene Methoden der Regularisierung in hoch-parametrischen Modellen vor. In der sehr populären Disziplin des Machine Learnings wird das hier besprochene Problem als die Selektion von relevanten Features im Kontext des Supervised Learnings dargestellt. 1.1 Beschreibung des Problems Alle die soeben genannten Formulierungen beschreiben das gleiche Problem. Wir gehen von einem Datensatz aus, welcher aus Beobachtungen besteht. Jede Beobachtung ist charakterisiert durch sehr viele unabhängige Grössen. Die Gewichtung der zu einer Beobachtung gehörenden Grössen wird über unbekannte Parameter erreicht. Als Beispiel für einen solchen Datensatz können wir eine Population mit SNP-typisierten Tieren betrachten. Das Typisierrungsergebnis für ein bestimmtes Tier enthält die Genotypen an den Genorten, welche bei der Typisierung untersucht werden. Die einzelnen Genorte werden als sogenannte Single Nucleotide Polymorphisms (SNP) bezeichnet. In Abhängigkeit des anbietenden Labors gibt es verschiedene Optionen für die gewünschte Typisierung. Die Optionen unterscheiden sich vor allem in der Dichte der untersuchten Genorte. Das heisst bei einer grösseren Dichte werden mehr SNPs untersucht. Typische Werte von gängigen Anbietern bewegen sich im Bereich zwischen 50000 (50K) bis rund 800000 (800K) untersuchte SNPs pro untersuchtes Genom. Die totale Anzahl an SNP im Genom beträgt rund 20 Millionen. Somit ist ein Typisierungsergebnis eine vom Anbieter gemachte Auswahl aller verfügbaren SNPs. 1.2 Rückblick Bis Anfangs des 21. Jahrhundert wurden eigentlich keine genomischen Informationen in Zuchtprogrammen berücksichtigt. Mit genomischer Information ist hier die Genotyp-Varianten einer grosser Anzahl von Genorten, welche über das ganze Genom verteilt ist. Um die Jahrtausendwende waren sehr viele ForscherInnen in einem Gebiet aktiv, welches damals als Mapping von sogenannten Quatitative Trait Loci (QTL) bezeichnet wurde. Eine Übersicht zu QTL ist im Buch (Balding, Bishop, and Cannings 2009). Das Ziel der Untersuchungen im Bereich QTL-Mapping war das Finden von Regionen im Genom, welche wichtig sind für die Ausprägung von spezifischen Phänotypen. Heute spricht man nicht mehr QTL-Mapping sondern heute wird die Suche von genetischen Orten, welche einen wichtigen Einfluss auf die Ausprägung eines Phänotyps haben, mit Genome Wide Association Study (GWAS) bezeichnet. Trotz umfangreicher Forschungstätigkeit auf dem Gebiet des QTL-Mappings, fanden keine Resultate aus diesen Arbeiten den Weg in die praktische Zuchtarbeit. Somit verläuft die Zuchtarbeit bis vor kurzem nach dem klassischen Schema, welches nachfolgend gezeigt ist. 1.2.1 Paradigmenwechsel Die Publikation (Meuwissen, Hayes, and Goddard 2001) gilt als Grundstein für eine neue Ära in der praktischen Zuchtarbeit. Die Autoren haben gezeigt, wie genomische Information, welche in genügender Dichte vorliegen muss, zur Schätzung von Zuchtwerten verwendet werden kann. Sie konnten auch statistische Methoden zeigen, mit welchen die Parameter in verwendeten Modell geschätzt werden können. Wir werden zu einem späteren Zeitpunkt noch genauer auf den Inhalt des Papers von (Meuwissen, Hayes, and Goddard 2001) zurückkommen. 1.2.2 Vor der genomischen Selektion Von Anfangs der 1980-er Jahre wurden die statistischen Auswertungen in den Zuchtprogrammen auf das BLUP-Tiermodell abgestellt. In dieser Zeit wurden die einfachen Modelle auch durch verschiedene Erweiterungen ausgebaut. Bei der Milchproduktion wurde von einfachen Laktationsleistungen auf Testtagesmodelle umgestellt. Bei der Wurfgrösse beim Schwein oder anderen diskreten Merkmalen wurden auch Generalized Linear Mixed Models (GLMM) verwendet. Unabhängig von den verwendeten Modellen wurden in allen Auswertungen die gleichen Informationen berücksichtigt. - phänotypische Leistungen - Pedigree - Varianzkomponenten aus periodischen Schätzungen Versuchsweise wurde ab den 1990-er Jahren erste genetische Marker mit in den Zuchtprogrammen berücksichtigt. Das Problem war dass diese wenigen Markern sehr schnell auf einer bestimmten Variante fixiert war. Nach der Fixierung lieferten diese Genorte keine zusätzliche Information zur Auswahl von potentiellen Zuchttieren. Es war zu dieser Zeit nicht klar, wie das Problem der Fixierung von einzelnen Genorten behandelt werden soll und es gab auch keine wirklich gute Strategie für die Berücksichtigung von genetischen Informationen in Zuchtprogrammen. 1.2.3 Modellierung vor der genomischen Selektion Vor der Einführung der genomischen Selektion war das BLUP-Tiermodell die Methode der Wahl für die Auswertung von Leistungsdaten in der Tierzucht. In seiner einfachsten Form sieht dieses Modell wie folgt aus. \\[\\begin{equation} y = Xb + Zu + e \\end{equation}\\] Die Co-Varianzen der zufälligen Komponenten sind definiert als: \\[Var(\\mathbf{e}) = \\mathbf{R} = \\mathbf{I}*\\sigma_e^2\\] \\[Var(\\mathbf{u}) = \\mathbf{G} = \\mathbf{A} * \\sigma_g^2\\] \\[Cov(\\mathbf{u},\\mathbf{e}^T) = Cov(\\mathbf{e}, \\mathbf{u}^T) = \\mathbf{0}\\] \\[\\rightarrow Var(\\mathbf{y}) = \\mathbf{V} = \\mathbf{ZGZ}^T + \\mathbf{R}\\] 1.3 Genomische Selektion Vom Standpunkt der Genetik aus basiert das BLUP-Tiermodell auf dem sogenannten Infinitesimalmodell. In diesem Modell wird angenommen, dass die phänotypische Ausprägung eines Merkmals durch die Summe von unendlich vielen Genorten mit undendlich kleiner Wirkung verursacht wird. Durch diese Annahme lässt sich dem einzelnen Tier kein fix definierter Genotyp mehr zuordnen. Diese fehlende Zuordnung der einzelnen Genotypen wird über die Modellierung der Zuchtwerte als zufällige Effekte gelöst. Die zufälligen Effekte der Zuchtwerte entsprechen dabei Realisierungen einer Zufallsvariablen mit vorgegebener Verteilung. In der genomischen Selektion verwenden wir das polygene Modell. Dabei werden die phänotypischen Leistungen als Summe von bekannten Genorten zusammengesetzt. Die konkrete Umsetzung des polygenen Modells wurde zum ersten Mal im Paper von (Meuwissen, Hayes, and Goddard 2001) gezeigt. Diese Autoren haben aufgrund von simulierten Daten gezeigt, dass es mit Hilfe einer sehr dichten Markerkarte möglich ist, die phänotypischen Leistungen alleine aufgrund der geschätzten Wirkungen an den Markergenorten zu modellieren. Die folgende Abbildung fasst die Unterschiede zwischen dem Infinitesimalmodell und dem polygenen Modell zusammen. 1.3.1 Modellierung Im Zusammenhang mit der genomischen Selektion besteht die Modellierung der Daten aus zwei Komponenten Die Schätzung der Gen-Wirkungseffekte (\\(a\\)) Die Schätzung der genomischen Zuchtwerte Die Umsetzung der beiden Komponenten wird in zwei verschiedenen Verfahren gemacht. Im Zwei-Schritt-Verfahren werden beide Komponenten einzeln an verschiedenen Teilen der Zuchtpopulation ausgeführt. Im Gegensatz dazu werden im Single-Step-Verfahren beide Komponenten im gleichen Schritt realisiert. 1.3.2 Zwei-Schritt-Verfahren Beim Zwei-Schritt-Verfahren wird die Population in ein Trainings- und ein Testset unterteilt. Im Trainingsset werden aufgrund von Typisierungsergebnissen und Beobachtungen die Gen-Wrkungseffekte (\\(a\\)) geschätzt. Sobald die Schätzwerte für die \\(a\\)-Effekte bekannt sind können diese für die Schätzung der genomischen Zuchtwerte verwendet werden. Da aufgrund der Typisierungsergebnisse die Genotypen an den SNP-Genorten bekannt sind, brauchen wir kein gemischtes lineares Modell mehr. Im Gegensatz zur BLUP-Zuchtwertschätzung, ist in der genomischen Selektion beim Zwei-Schritt-Verfahren ein einfaches lineares Modell ausreichend. Im Idealfall, wenn die komplette Information zu allen Gen-Wirkungseffekten (\\(a\\)) bekannt sind, dann setzen sich die genotypischen Werte einfach zusammen aus den aufsummierten \\(a\\)-Werten. In Matrix-Vektor-Schreibweise können wir die folgende Modellgleichung aufstellen. \\[\\begin{equation} g = 1\\mu + Ma + \\epsilon \\end{equation}\\] Die Matrix \\(M\\) ist eine Inzidenzmatrix, welche die genotypischen Werte im Vektor \\(g\\) mit den Gen-Wirkungseffekten \\(a\\) verknüpft. Die Matrix \\(M\\) hat die Dimension \\(n\\times p\\) wobei \\(n\\) der Anzahl Individuen mit einem Typisierungsergebnis entspricht und \\(p\\) gleich der Anzahl SNP-Genorte ist. In der Realität im ersten Schritt des Zwei-Schritt-Verfahrens kennen wir aber weder die Komponenten des Vektors \\(g\\) noch die Gensubstitutionseffekte \\(a\\). Somit müssen wir das Modell zur Schätzung der \\(a\\)-Effekte modifizieren. Bei der aktuellen Modifikation ersetzen wir den Vektor \\(g\\) durch die phänotypischen Beobachtung \\(y\\). \\[\\begin{equation} y = (1\\mu + Xb) + Ma + (\\epsilon+ e) \\end{equation}\\] Das Modell mit den phänotypischen Beobachtungen erlaubt eine Schätzung der \\(a\\)-Effekte. Mit diesem Ansatz gibt es aber zwei Probleme. Verfügbarkeit: wirtschaftliche Merkmale wie Milchleistung sind nur beim weiblichen Geschlecht beobachtbar. Somit müsste für die Selektion auf der männlichen Seite wieder auf Nachkommenleistungen zurückgegriffen werden. Dies verlängert aber das Generationenintervall. Vergleichbarkeit: Beim Austausch von Information zwischen verschiedenen Ländern sind die phänotypischen Leistungen nicht unbedingt vergleichbar. Diese beiden Probleme können gelöst werden, wenn anstelle von phänotypischen Leistungen \\(y\\), geschätzte Zuchtwerte \\(\\hat{g}\\) verwendet werden. Das entsprechende Modell sieht dann wie folgt aus. \\[\\begin{equation} \\hat{g} = g + (\\hat{g} - g) = 1\\mu + Ma + (\\epsilon + (\\hat{g} - g) ) \\end{equation}\\] 1.3.3 Eigenschaften von BLUP-Zuchtwerten Aufgrund der Eigenschaften von den BLUP-Zuchtwerten \\(\\hat{g}\\) führt die Addition der Abweichung \\((\\hat{g} - g)\\) zu einer Reduktion der Varianz. Die Reduktion der Varianz bedeutet, dass \\(var(\\hat{g}) \\le var(g)\\) ist. Für BLUP-Zuchtwerte gilt, dass die Covarianz zwischen wahrem und geschätztem Zuchtwert gleich der Varianz der geschätzten Zuchtwerte ist. In Formeln geschrieben bedeutet dass, \\[\\begin{equation} cov(\\hat{g},g) = var(\\hat{g}) \\end{equation}\\] Setzen wir diese Beziehung in die Varianz der Abweichung \\((\\hat{g} - g)\\) ein, dann erhalten wir \\[\\begin{equation} var(\\hat{g} - g) = var(\\hat{g}) + var(g) - 2cov(\\hat{g},g) = var(g) - var(\\hat{g}) \\ge 0 \\end{equation}\\] Somit gilt, dass \\(var(g) \\ge var(\\hat{g})\\) und somit ist die Reduktion der Varianz gezeigt. Im Zusammenhang mit der Varianzreduktion steht auch die zweite Eigenschaft von BLUP-Zuchtwerten, welche uns hier Schwierigkeiten bereitet und zwar handelt es sich dabei um den sogenannten Shrinkage-Effekt. Für einen geschätzten Zuchtwert eines Tieres \\(i\\) bedeutet das, dass dieser zum Durchschnitt der geschätzten Zuchtwerte der Eltern regressiert wird. Das Ausmass dieses Regressions-Effektes hängt davon ab, aufgrund welcher Informationen der Zuchtwert von Tier \\(i\\) geschätzt wurde. Diese Abhängigkeit wird in der Zerlegung des geschätzten BLUP-Zuchtwertes des Tieres \\(i\\) in seine Komponenten sichtbar. Diese Zerlegung ist in (Hofer 1990) und in (von Rohr 2016) erklärt. Das Resultat der Zerlegung ist in der nachfolgenden Formel zusammengefasst. \\[\\begin{eqnarray} \\hat{g}_i &amp;=&amp; \\frac{1}{1 + \\alpha \\delta^{(i)} + {\\alpha\\over 4} \\sum_{j=1}^n \\delta^{(k_j)}} \\left[y_i - \\hat{\\mu} + {\\alpha\\over 2}\\left\\{\\delta^{(i)}(\\hat{g}_s + \\hat{g}_d) + \\sum_{j=1}^n \\delta^{(k_j)} (\\hat{g}_{k_j} - {1\\over 2}\\hat{g}_{l_j}) \\right\\} \\right] \\label{eq:AhatDecompEq} \\end{eqnarray}\\] Die Zerlegung des geschätzen Zuchtwertes \\(\\hat{g}_i\\) für Tier \\(i\\) zeigt die Abhängigkeit des Ausmasses der Regression von \\(\\hat{g}_i\\) auf den Durchschnitt der geschätzten Elternzuchtwerte \\(\\hat{g}_s\\) und \\(\\hat{g}_d\\). Hat das Tier \\(i\\) keine Eigenleistung \\(y_i\\), keine Nachkommen und keine Paarungspartner, so ist \\(\\hat{g}_i\\) vollständig durch \\(\\hat{g}_s\\) und \\(\\hat{g}_d\\) bestimmt. Sobald aber Tier \\(i\\) eine Eigenleistung hat und später dann noch Nachkommenleistungen dazukommen, nimmt der Einfluss von \\(\\hat{g}_s\\) und \\(\\hat{g}_d\\) auf \\(\\hat{g}_i\\) ab. Damit verringert sich auch das Ausmass des Regressions-Effektes von \\(\\hat{g}_i\\) auf den Durchschnitt der geschätzten Elternzuchtwerte. Durch die Berücksichtigung zusätzlicher Informationen, wie Eigenleistung und Leistungen von Nachkommen und Paarungsparter, bei der Schätzung des Zuchtwertes für Tier \\(i\\) steigt auch die Genauigkeit oder das Bestimmtheitsmass (\\(B\\)) des geschätzten Zuchtwertes. Wir können aufgrund der Eigenschaften von BLUP-Zuchtwerten können wir folgende Zusammenhänge aufstellen. Je grösser die verfügbare Information für die Schätzung eines Zuchtwertes für Tier \\(i\\), desto grösser ist das Bestimmtheitsmass des geschätzten Zuchtwertes und je tiefer ist der Regressions-Effekt des geschätzten Zuchtwertes auf den Durchschnitt der geschätzten Zuchtwerte der Eltern und je geringer ist auch die Varianzreduktion. 1.3.4 Einsatz von BLUP-Zuchtwerten in der genomischen Selektion Eigenschaften von BLUP-Zuchtwerten führen zu Varianzreduktion und dazu dass geschätzte Zuchtwerte zum Durchschnitt der geschätzten Zuchtwerte der Eltern regressiert werden. Diese beiden Effekte sind problematisch bei der Verwendung von BLUP-Zuchtwerten für die Schätzung der \\(a\\)-Effekte in der genomischen Selektion. Ein bestimmtes Tier \\(i\\) hat immer die gleichen SNP-Genotypen und wir gehen davon aus, dass diese auch immer die gleiche Wirkung auf die Ausprägung eines Phänotyps haben. Der mit BLUP geschätzte Zuchtwert eines Tieres ändert sich aber während seines Lebens. In der Zeitperiode der Geburt bis zur Beobachtung einer Eigenleistung ist der geschätzte Zuchtwert durch die geschätzten Zuchtwerte der Eltern bestimmt. Mit zunehmendem Alter werden für Tier \\(i\\) mehr Informationen in der Zuchtwertschätzung berücksichtigt. Somit ändert sich der geschätzte Zuchtwert und damit würde sich auch die aufgrund der BLUP-Zuchtwerte geschätzten \\(a\\)-Effekte ändern. Das ist aufgrund von unserer Annahme der konstanten Wirkung der \\(a\\)-Effekte ein unerwünschtes Verhalten. Die unerwünschten Veränderungen der geschätzten BLUP-Zuchtwerte werden durch eine Prozedur namens Deregression korrigiert. Da sich die Veränderungen der Zuchtwerte im wesentlichen durch eine Funktion der Änderungen im Bestimmtheitsmass beschrieben werden kann, ist die Deregression als Korrektur von geschätzten Zuchtwerten aufgrund deren Bestimmtheitsmass definiert. Einzelheiten zur Deregression können dem Paper (Garrick, Taylor, and Fernando 2009) entnommen werden. 1.4 Zusammenfassung Die deregressierten Zuchtwerten werden als Beobachtunen für die Schätzung der \\(a\\)-Effekte im ersten Schritt des Zwei-Schritt-Verfahrens verwendet. Die geschätzen \\(a\\)-Werte werden dann verwendet um im zweiten Schritt die genomischen Zuchtwerte der restlichen Population zu berechnen. Die im Zwei-Schritt-Verfahren verwendeten Modelle zur Schätzung der \\(a\\)-Effekte sind einfache lineare Modelle. Die Anzahl der Parameter \\(p\\) in diesen Modellen entspricht der Anzahl zu schätzender \\(a\\)-Werte und somit der Anzahl an SNPs pro Typisierung. Diese Anzahl ist typischerweise bei 50K kann aber auch bis 800K anwachsen. In den meisten Fällen ist \\(p &gt;&gt; n\\), wenn \\(n\\) die Anzahl typisierter Tiere ist. Somit können wir das klassische Least Squares Verfahren für die Schätztung der Parameter nicht verwenden. 1.5 Ausblick Das Problem \\(p &gt;&gt; n\\) kommt heutzutage in sehr vielen Anwendungen vor. In den nachfolgenden Kapiteln wollen wir uns ein paar Lösungsansätze anschauen, welche uns trotz der spärlich verfügbaren Informationen in den hoch-dimensionalen Parameterräumen, sinnvolle Schätzwerte für die Parameter im Modell liefern kann. References "],
["linreg.html", "Kapitel 2 Multiple Lineare Regression 2.1 Beispiele für Lineare Regressionen 2.2 Methode der kleinsten Quadrate (Least Squares) 2.3 Eigenschaften der Schätzungen 2.4 Tests und Vertrauensintervalle 2.5 Output von R 2.6 Analyse der Residuen und Überprüfung der Modellannahmen 2.7 Selektion eines Modells", " Kapitel 2 Multiple Lineare Regression Das Material dieses Kapitels ist eine Zusammenfassung aus den Vorlesungsunterlagen von (Bühlmann and Mächler 2014). Die multiple lineare Regression ist wie folgt definiert. Jedes Individuum \\(i\\) oder jedes Objekt \\(i\\) in einem Datensatz ist charakterisiert durch eine Zielgrösse \\(y_i\\) und durch eine Menge von erklärenden Variablen \\(\\left\\{x_{i,1}, x_{i,2}, \\ldots, x_{i,p}\\right\\}\\). Zusammengefasst besteht die bekannte Information für jedes Individuum oder jedes Objekt \\(i\\) aus einem Datensatz aus der folgenden Menge \\[\\left\\{x_{i,1}, x_{i,2}, \\ldots, x_{i,p}, y_i\\right\\}\\] Das multiple lineare Regressionsmodell versucht die Zielgrösse bis auf einen zufälligen Restterm \\(\\epsilon\\) als lineare Funktion der erklärenden Variablen auszudrücken. Unser Ziel besteht in der Schätzung der unbekannten Parameter, welche im Regressionsmodell enthalten sind. Die nachfolgend gezeigte Modellformel soll die Unterscheidung zwischen erklärenden Variablen und unbekannten Parametern verdeutlichen. \\[\\begin{equation} y_i = \\beta_i x_{i,1} + \\ldots + \\beta_p x_{i,p} + \\epsilon_i \\qquad (i = 1, \\ldots, n) \\label{eq:MultLinRegForm} \\end{equation}\\] Fassen wir die Gleichungen über alle \\((i = 1, \\ldots, n)\\) zusammen und verwenden die Matrix-Vektor-Notation, so sieht das lineare Modell in () wie folgt aus. \\[\\begin{equation} y = X\\beta + \\epsilon \\label{eq:MultLinRegMatVec} \\end{equation}\\] Die Reste \\(\\epsilon_i\\) im Modell () haben wir als zufällige Effekte definiert. Somit müssen wir geeignete Annahmen zur Dichteverteilung der \\(\\epsilon_i\\) treffen. Meistens gehen wir davon aus, dass die \\(\\epsilon_i\\) unabhängig sind und der gleichen Verteilung folgen. In der englischsprachigen Literatur wird das mit dem Begriff independent, identically distributed (i.i.d.) bezeichnet. Der Erwartungswert und die Varianz der Zufallsvariablen \\(\\epsilon\\) sind \\(E\\left[\\epsilon_i \\right] = 0\\) und \\(Var(\\epsilon_i) = \\sigma^2\\). 2.1 Beispiele für Lineare Regressionen 2.1.1 Regression mit Achsenabschnitt Die erste erklärende Variable wir oft als eine Konstante angenommen. Das bedeutet, dass der erste Kolonnenvektor in der Matrix \\(X\\) gleich dem Eins-Vektor ist. Die konstante erklärende Variable erlaubt es einen sogenannten Achsenabschnitt anzupassen. In skalarer Schreibweise hat das lineare Modell mit Achsenabschnitt die folgende Form \\[\\begin{equation}y_i = \\beta_1 + \\beta_2x_{i2} + \\ldots + \\beta_px_{ip} + \\epsilon_i \\qquad (i = 1,\\ldots,n)\\end{equation}\\] 2.1.2 Regression durch den Ursprung Im Gegensatz zur Regression mit Achsenabschnitt steht die Regression durch den Ursprung. Diese kennt keine konstante erklärende Variable. Das Modell ohne Achsenabschnitt sieht dann wie folgt aus. \\[\\begin{equation}y_i = \\beta_1x_{i1}+ \\ldots + \\beta_px_{ip} + \\epsilon_i \\qquad (i = 1,\\ldots,n)\\end{equation}\\] 2.1.3 Regression mit transformierten Variablen Regressionen können auch auf Transformationen der erklärenden Variablen oder auf transformierte Zielgrössen angepasst werden. Als Beispiel verwendet die sogenannte “quadratische” Regression die \\(x_{ij}\\) und die \\(x_{ij}^2\\) als erklärende Variablen. Das Modell entspricht dann einer quadratischen Funktion in den \\(x_j\\) ist aber immer noch eine lineare Funktion im Bezug auf die Parameter \\(\\beta_j\\). \\[\\begin{equation}y_i = \\beta_1 + \\beta_2 x_{i2} + \\beta_3 x_{i2}^2 + \\epsilon_i \\qquad (i = 1,\\ldots,n)\\end{equation}\\] Abgesehen von der quadratischen Regression sind auch andere Arten von Transformationen der erklärenden Variablen denkbar. Ein Beipsiel ist in der folgenden Gleichung gezeigt. \\[\\begin{equation}y_i = \\beta_i + \\beta_2 \\log(x_{i2}) + \\beta_3 sin(\\pi x_{i3}) + \\epsilon_i \\qquad (i = 1,\\ldots,n)\\end{equation}\\] Auch dieses Modell ist linear in den Parametern \\(\\beta_j\\) und wird somit als lineare Regression bezeichnet. 2.1.4 Anwendungen in den Nutztierwissenschaften Eine Anwendung der linearen Regression in den Nutztierwissenschaften ist die Schätzung vom Lebendgewicht von Tieren aufgrund des Brustumfangs. Dafür werden Messbänder verwendet, welche auf der einen Seite den Brustumfang angeben und auf der anderen Seite das geschätzte Körpergewicht. Diese Anwendung macht eine Voraussage der Zielgrösse Körpergewicht aufgrund der beobachteten erklärenden Variablen Brustumfang. Damit eine Voraussage für die Zielgrösse aufgrund der erklärenden Variablen möglich ist, muss zuerst ein angemessener Datensatz vorliegen, in welchem man für jedes Tier beide Informationen, also sowohl Körpergewicht als auch Brustumfang bekannt ist. Aufgrund dieser Informationen können dann die unbekannten Parameter geschätzt werden. Die geschätzten Parameter werden dann für die Vorhersagen verwendet. Bei diesem ersten Beispiel handelt es sich um eine einfache lineare Regression. Das verwendete Regressionsmodelle hat nur eine erklärende Variable (Brustumfang) und eine Zielvariable (Gewicht). Das zu dieser Anwendung zugehörige Modell lautet \\[\\begin{equation}y_{G,i} = \\beta_1 + \\beta_2 x_{B,i} + \\epsilon_i\\end{equation}\\] 2.1.5 Ziele der linearen Regression Gute Anpassung: das Modell soll so sein, dass die erklärenden Variablen möglichst präzise Voraussagen zu den Zielvariablen machen. Das Standardtool für die Anpassung ist die Methode der kleinsten Quadrate (Least Squares). Parameterschätzung: die unbekannten Parameter sollen so geschätzt sein, dass eine Veränderung der erklärenden Variablen in einer entsprechenden Veränderung der Zielgrösse führt. Vorhersage: noch nicht beobachtete Zielgrössen sollen als Funktionen von erklärenden Variablen vorhergesagt werden können Fehler und Signigikanz: werden durch Vertrauensintervalle und statistische Tests beurteilt Modellentwicklung: ist ein interaktiver Prozess, welche durch die oben genannten Ziele beeinflusst wird 2.2 Methode der kleinsten Quadrate (Least Squares) Gegeben sei das lineare Modell \\(y = X\\beta + \\epsilon\\). Wir wollen eine, gemäss den oben formulierten Zielen, möglichst gute Schätzung für \\(\\beta\\) finden. Die folgende Darstellung erklärt, wie die Methode der kleinsten Quadrate funktioniert. Die Punkte stehen für die Beobachtungen \\(y_i\\). Die rote Linie steht für die Regressionsgerade. Die Distanz des Punktes zur Projektion in Richtung der \\(y\\)-Achse auf der Regressionslinie entspricht dem Residuum \\(r_i = y_i - x_i^T \\hat{\\beta}\\). Für eine bestimmte Regressionsgerade (rote Linie im Diagramm) wird für jeden Punkt \\(y_i\\) das entsprechende Residuum \\(r_i\\) berechnet. Die Residuen \\(r_i\\) werden quadriert und addiert. Diese summierten Quadrate der Residuen stellt ein Mass dar, wie gut die Regressionsgerade an die Beobachtungspunkte \\(y_i\\) angepasst ist. Position und Verlauf der Regressionsgeraden können durch die Wahl des Vektors \\(\\beta\\) beeinflusst werden. Gemäss der Methode der kleinsten Quadrate soll \\(\\beta\\) so bestimmt werden, dass die Summe der quadrierten Residuen minimal wird. Der so bestimmte Vektor \\(\\beta\\) wird dann als Least-Squares-Schätzer bezeichnet. In einer Formel können wir die Berechnung des Least-Squares-Schätzers (\\(\\hat{\\beta}\\)), wie folgt ausdrücken. \\[\\begin{equation}\\hat{\\beta} = argmin_{\\beta} \\| y - X\\beta \\| ^2\\end{equation}\\] wobei \\(\\| .\\|\\) für die Euklidsche Norm oder die Euklidsche Distanz steht. In einem ersten Schritt geht es darum das Minimum für den Ausdruck \\(\\| y - X\\beta \\| ^2\\) zu finden. Dabei ist es einfacher, wenn wir folgende Umformung verwenden. \\[\\begin{equation}\\| y - X\\beta \\| ^2 = (y - X\\beta)^T(y - X\\beta) = y^Ty - y^TX\\beta - \\beta^TX^Ty + \\beta^TX^TX\\beta\\end{equation}\\] Leiten wir diesen Ausdruck nach \\(\\beta\\) ab und setzen die erste Ableitung gleich \\(0\\), dann erhalten wir eine Gleichung für den Least-Squares-Schätzer \\(\\hat{\\beta}\\). \\[\\begin{equation}-y^TX - y^TX + 2\\hat{\\beta}^TX^TX = 0\\end{equation}\\] Aus der obigen Formel können wir die sogenannte Normalgleichung herleiten. Diese lautet \\[\\begin{equation}X^TX\\hat{\\beta} = X^Ty\\end{equation}\\] Unter der Annahme, dass die Matrix \\(X\\) vollen Kolonnenrang \\(p\\) hat, können wir explizit nach \\(\\hat{\\beta}\\) auflösen. \\[\\begin{equation}\\hat{\\beta} = (X^TX)^{-1}X^Ty\\end{equation}\\] Die Residuen \\(r_i = y_i - x_i^T\\hat{\\beta}\\) sind Schätzungen für die Resteffekte \\(\\epsilon_i\\) und können somit für die Schätzung von \\(\\sigma^2\\) verwendet werden. \\[\\begin{equation}\\hat{\\sigma}^2 = \\frac{1}{n-p}\\sum_{i=1}^{n} r_i^2\\end{equation}\\] Der Faktor \\(1/(n-p)\\) scheint ungewöhnlich, aber es kann gezeigt werden, dass die Wahl dieses Faktors zur Erwartungstreue von \\(\\hat{\\sigma}^2\\) führt. Das heisst, es gilt \\(E\\left[ \\hat{\\sigma}^2 \\right] = \\sigma^2\\). 2.2.1 Annahmen hinter dem linearen Modell Abgesehen davon, dass die Matrix \\(X\\) vollen Kolonnenrang \\(p&lt;n\\) haben muss, wurden für die erklärenden Variablen keine Annahmen getroffen. Insbesondere können die erklärenden Variablen kontinuierlich oder diskret sein. Kontinuierliche Variablen sind typischerweise Messgrössen, welche als reelle Zahlen (Gleitkommazahlen) erhoben werden. Diskrete Grössen können nur bestimmte Werte, wie zum Beispiel \\(0\\) oder \\(1\\) annehmen. Damit die Anpassung eines linearen Modells mit der Methode der kleinsten Quadrate Sinn macht und die Tests und Vertrauensintervalle gültig sind, müssen wir gewisse Annahmen treffen. Korrektheit des linearen Modells: Das heisst \\(E\\left[\\epsilon_i \\right] = 0\\) für alle \\(i\\). Das heisst aber auch, dass die Zielgrössen und die erklärenden Variablen nicht gemischt werden dürfen. Alle \\(x_i\\) sind exakt: Es wird angenommen, dass die Werte für \\(x_i\\) ohne Fehler beobachtet werden können. Konstante Varianz der Resteffekte: \\(Var(\\epsilon_i) = \\sigma^2\\) für alle \\(i\\) Resteffekte sind unkorreliert: \\(Cov(\\epsilon_i, \\epsilon_j) = 0\\) für alle \\(i\\ne j\\) Resteffekte folgen Normalverteilung: Der Vektor \\(\\epsilon\\) der Resteffekte folgt einer multivariaten Normalverteilung. Falls diese Annahmen verletzt sind, gibt es eine Reihe von Massnahmen, welche getroffen werden können. Bei Verletzung der Annahme 3, können “weighted least squares” Methoden verwendet werden. Ähnlich bei Verletzung der Annahme 4, können wir “generalized least squares” verwenden. Ist die Annahme 5 der Normalverteilung nicht erfüllt, können wir auf sogenannte “robuste Methoden” ausweichen. Falls Annahme 2 nicht zutrifft, braucht es Korrekturen, welche als “errors in variables” bezeichnet wird. Falls die Annahme 1 nicht stimmt, braucht es nicht-lineare Modelle. Die folgende Grafik zeigt das Beispiel des sogenannten “Pillen-Knicks”. Dabei werden die Anzahl Geburten seit 1930 in der Schweiz gezeigt. Hier sind die Annahmen 1 und 4 verletzt. Dieses Beispiel zeigt auch die Gefahr bei Vorhersagen in Bereiche, wo keine erklärende Variablen vorliegen. 2.2.2 Kein Ersatz der multiplen Regression durch mehrere einfache Regressionen Eine multiple Regression (mit mehreren erklärenden Variablen) soll nicht durch mehrere einfache Regressionen (mit nur einer erklärenden Variablen) ersetzt werden. Das folgende simulierte Beispiel zeigt weshalb. Wir betrachten die folgenden erklärenden Variablen \\(x^{(1)}\\) und \\(x^{(2)}\\) und die Zielgrösse \\(y\\) mit folgenden Werten x1 x2 y 0 -1 1 1 0 2 2 1 3 3 2 4 0 1 -1 1 2 0 2 3 1 3 4 2 Die multiple Regression führt zur Lösung der kleinsten Quadrate, welche die Daten exakt beschreibt, so wie diese erzeugt wurden. \\[\\begin{equation}y_i = \\hat{y_i} = 2x_{i1} - x_{i2} \\qquad \\text{für alle } i \\text{ mit } \\hat{\\sigma}^2 = 0\\end{equation}\\] Wird an die Daten nur eine einfache Regression mit der erklärenden Variablen \\(x^{(2)}\\) und ignoriert \\(x^{(1)}\\), so erhalten wir das folgende Resultat \\[\\begin{equation}\\hat{y_i} = {1\\over 9}x_{i2} + {4\\over 3} \\qquad \\text{für alle } i \\text{ mit } \\hat{\\sigma}^2 = 1.72\\end{equation}\\] Der Grund dafür ist, dass die erklärenden Variablen \\(x^{(1)}\\) und \\(x^{(2)}\\) korreliert sind. Falls \\(x^{(1)}\\) steigt, dann steigt auch \\(x^{(2)}\\). Da aber in der multiplen Regression \\(x^{(1)}\\) einen grösseren Koeffizienten hat als \\(x^{(2)}\\), muss dieser Effekt in der einfachen Regression durch \\(x^{(2)}\\) kompensiert werden. Dies führt zur Abweichung zwischen den Resultaten der beiden Analysen. 2.3 Eigenschaften der Schätzungen Die Least-Squares-Schätzer sind Zufallsvariablen, da für jeden Datensatz den wir vom gleichen unterliegenden Prozess beobachten, andere Werte resultieren. Damit ändern sich auch die Least-Squares-Schätzer. Da die Schätzer Funktionen der beobachteten Daten sind, haben die Schätzer auch einen zufälligen Charakter. Somit können wir Eigenschaften betreffend den Verteilungen und den Momenten für die Least-Squares-Schätzer herleiten. Die Ergebnisse sind hier nur kurz zusammengefasst. 2.3.1 Momente der Least-Squares Schätzungen Wir nehmen das folgende lineare Modell an \\[\\begin{equation}y = X\\beta + \\epsilon \\text{, } E\\left[\\epsilon \\right] = 0 \\text{, } Cov(\\epsilon) = E\\left[\\epsilon\\epsilon^T \\right] = \\sigma^2I_{n\\times n}\\end{equation}\\] Zusammen mit den oben getroffenen Annahmen können wir folgende Aussagen machen \\(E\\left[\\hat{\\beta}\\right] = \\beta\\), das heisst, \\(\\hat{\\beta}\\) ist unverzerrt \\(E\\left[\\hat{y}\\right] = E\\left[y\\right] = X\\beta\\), was aus 1. folgt. Zudem ist, \\(E\\left[r\\right] = 0\\) \\(Cov(\\hat{\\beta}) = \\sigma^2(X^TX)^{-1}\\) \\(Cov(\\hat{y}) = \\sigma^2P\\), \\(Cov(r) = \\sigma^2(I-P)\\) Die Matrix \\(P\\) ist definiert als Projektionsmatrix aus \\(\\hat{y} = Py\\). Setzen wir die Least-Squares-Schätzer ein, dann folgt \\[\\begin{equation}\\hat{y} = X\\hat{\\beta} = X(X^TX)^{-1}X^Ty = Py\\end{equation}\\] Somit ist die Matrix \\(P\\) definiert als \\(P=X(X^TX)^{-1}X^T\\). 2.3.2 Verteilung der Least-Squares-Schätzer unter normalverteilten Fehlern Zusätzlich zum linearen Modell nehmen wir an, dass \\(\\epsilon_i, \\ldots, \\epsilon_n \\text{ i.i.d. } \\mathcal{N}(0,\\sigma^2)\\), dann können wir zeigen, dass \\(\\hat{\\beta} \\sim \\mathcal{N}_p\\left(\\beta, \\sigma^2(X^TX)^{-1}\\right)\\) \\(\\hat{y} \\sim \\mathcal{N}_n\\left(X\\beta,\\sigma^2P \\right)\\), \\(r \\sim \\mathcal{N}_n\\left(0,\\sigma^2(I-P) \\right)\\) \\(\\hat{\\sigma}^2 \\sim \\frac{n}{n-p}\\chi_{n-p}^2\\) Die Annahme der Normalverteilung ist oft (annähernd) erfüllt und kann durch den zentralen Grenzwertsatz bei grösseren Datensätzen begründet werden. Diese Eigenschaften im Bezug auf die Verteilung der Schätzer führt zur Herleitung von Vertrauensintervallen und statistischen Tests für die geschätzten Parameter. Sind die Annahmen der Normalverteilung nicht erfüllt, müssen wir auf sogenannte robuste Methoden ausweichen. Diese werden hier nicht behandelt. 2.4 Tests und Vertrauensintervalle 2.4.1 Einzeltests Wir nehmen an, dass das lineare Modell korrekt ist und dass die Resteffekte \\(\\epsilon_1, \\ldots, \\epsilon_n \\text{ i.i.d. } \\sim \\mathcal{N}\\left(0, \\sigma^2 \\right)\\). Dann haben wir gesehen gemäss den Eigenschaften aus dem vorherigen Abschnitt ist dann \\(\\hat{\\beta}\\) normalverteilt. Im Allgemeinen sind wir daran interessiert, ob ein bestimmter Parameter \\(\\beta_j\\) einen Einfluss hat. Dies lässt sich mit der Nullhypothese \\(H_{0,j}: \\beta_j = 0\\) gegenüber der Alternativen \\(H_{A,j}: \\beta_j \\ne 0\\) überprüfen. Da \\(\\hat{\\beta}\\) einer Normalverteilung folgt, können wir herleiten, dass unter der Nullhypothese \\(H_{0,j}\\) gilt \\[\\begin{equation}\\frac{\\hat{\\beta_j}}{\\sqrt{\\sigma^2(X^TX)_{jj}^{-1}}} \\sim \\mathcal{N}(0,1)\\end{equation}\\] Da \\(\\sigma^2\\) unbekannt ist, ist die obige Teststatistk in der Praxis nicht brauchbar. Ersetzen wir \\(\\sigma^2\\) durch den Schätzwert \\(\\hat{\\sigma}^2\\) so erhalten wir die sogenannte t-Teststatistik. \\[\\begin{equation}T_j = \\frac{\\hat{\\beta_j}}{\\sqrt{\\hat{\\sigma}^2(X^TX)_{jj}^{-1}}} \\sim t_{n-p}\\end{equation}\\] Anhand dieses Tests können wir die Relevanz der erklärenden Variablen quantifizieren, indem wir die Teststatistiken \\(T_j\\) für \\(j=1,\\ldots,p\\) analysieren. Die Beurteilung der Relevanz der erklärenden Variablen aufgrund dieser einzelnen t-Tests birgt zwei Probleme. Multiples Testen: Werden sehr viele Tests durchgeführt, dann sind bei einem angenommenen Signifikanz-Niveau von \\(\\alpha\\) automatisch ein Anteil \\(\\alpha\\) aller Tests signifikant. Werden beispielsweise \\(100\\) Tests auf dem Niveau \\(\\alpha = 0.05\\) durchgeführt, dann sind automatisch \\(5\\) Tests signifikant. Korrelation der erklärenden Variablen: Falls die erklärenden Variablen untereinander korreliert sind, dann beeinflusst dies auch die Testergebnisse und kann diese verzerren. 2.4.2 Globaler Test Wenn wir testen wollen, ob (abgesehen vom Achsenabschnitt) überhaupt eine erklärende Variable einen Einfluss auf die Zielgrösse hat, dann können wir diese mit folgender Nullhypothese \\(H_0: \\beta_2 = \\ldots = \\beta_p = 0\\) versus die Alternative \\(H_A: \\beta_j \\ne 0\\) für \\(j=2,\\ldots, p\\) tun. Solch ein Test kann mit der Zerlegung der Varianz der Beobachtungen \\(y_i\\) um das globale Mittel \\(\\bar{y} = n^{-1}\\sum_{i=1}^ny_i\\) konstruiert werden. In Vektor-Schreibweise sieht diese Zerlegung wie folgt aus \\[\\begin{equation}\\|y - \\bar{y}\\|^2 = \\|\\hat{y} - \\bar{y}\\|^2 + \\|y - \\hat{y}\\|^2\\end{equation}\\] Diese Zerlegung teilt die quadrierten Abweichungen der Beobachtungen \\(y\\) vom allgemeinen Mittel \\(\\bar{y}\\) in die quadrierten Abweichungen der gefitteten Werte \\(\\hat{y}\\) vom allgemeinen Mittel plus die quadrierten Residuen \\(y-\\hat{y}\\) auf. Eine solche Zerlegung lässt sich am einfachsten in einer Varianzanalysetabelle zusammenfassen. Im Falle der globalen Nullhypothese haben die erklärenden Variablen keinen Einfluss auf die Zielgrösse. Somit ist \\(E\\left[y \\right] = const. = E\\left[\\bar{y}\\right]\\). Daraus folgt, dass der Erwartungswert der mittleren Summenquadrate der Regression gleich \\(\\sigma^2\\) ist. Teilen wir die mittleren Summenquadrate der Regression durch die mittleren Summenquadrate des Rests (Schätzung von \\(\\sigma^2\\)) erhalten wir eine dimensionslose Grösse, welcher einer \\(F\\)-Statistik entspricht. Unter der Nullhypothese gilt, dass \\[\\begin{equation}F = \\frac{\\|\\hat{y} - \\bar{y}\\|^2/(p-1)}{\\|y - \\hat{y}\\|^2/(n-p)} = F_{p-1,n-p}\\end{equation}\\] Dies wird als globaler \\(F\\)-Test der Regression bezeichnet. Abgesehen von der Bewertung der statistischen Signifikanz mit dem globalen \\(F\\)-Test, sind wir auch daran interessiert, wie gut die Anpassung des Modells an die Daten ist. Eine mögliche Grösse für die Qualität der Anpassung ist das sogenannten \\(R^2\\). Dies ist definiert als das folgende Verhältnis. \\[\\begin{equation}R^2 = \\frac{\\|\\hat{y} - \\bar{y}\\|^2}{\\|y - \\bar{y}\\|^2}\\end{equation}\\] Das \\(R^2\\) entspricht dem Verhältnis der Variation der Beobachtungen um das globale Mittel, welcher durch die Regression erklärt werden kann. Aus dieser Definition ist klar, dass wir nach Modellen suchen mit einem möglichst grossen \\(R^2\\). 2.4.3 Vertrauensintervalle In Anlehnung an den \\(t\\)-Test der einzelnen Parameter \\(\\beta_j\\) können wir Vertrauensintervalle ableiten. Das zwei-seitige Vertraunensintervall auf dem Niveau \\(1-\\alpha\\) für \\(\\beta_j\\) ist definiert als \\[\\begin{equation}\\hat{\\beta}_j \\pm \\sqrt{\\hat{\\sigma}^2(X^TX)_{jj}^{-1}} \\ \\cdot t_{n-p;1-\\alpha/2}\\end{equation}\\] Hier \\(t_{n-p;1-\\alpha/2}\\) ist das \\(1-\\alpha/2\\)-Quantil der \\(t_{n-p}\\)-Verteilung. 2.5 Output von R In R wird eine lineare Regression mit der Funktion lm() angepasst. Die Zusammenfassung der Resultate von lm() ist in der nachfolgenden Diagramm gezeigt. Die verschiedenen Bereiche der Resultate sind numeriert durch farbige Rechtecke gekennzeichnet. Im ersten Bereich ist unter der Überschrift Call der Funktionsaufruf nochmals aufgeführt. So ist dokumentiert, wie die nachfolgenden Resultate zustande kamen. Der zweite Bereich gibt einige Informationen zur empirischen Verteilung der Residuen. Diese Kennzahlen der Residuen-Verteilung sind nützlich um gewisse Annahmen bezüglich der Residuen im Modell grob überprüfen zu können. Block drei enthält die Resultate der Parameterschätzungen. Abgesehen von den Schätzwerten sind auch die Standardfehler und die Quantile des entsprechenden t-Tests für jeden Parameter enthalten. Die Kolonne ganz links im Block drei zeigt das Signifikanz-Niveau der t-Tests für jeden Parameterschätzwert. Der vierte und letzte Block enthält die Schätzung der Rest-Standardabweichung (residual standard error) und das Testergebnis des globalen F-Tests. Zur Beurteilung der Anpassungsqualität ist das \\(R^2\\) und eine korrigierte Version des \\(R^2\\) aufgeführt. Die korrigierte Version des \\(R^2\\) berücksichtigt die Anzahl der geschätzten Parameter und ist definiert als \\[\\begin{equation}\\bar{R}^2 = R^2 - (1-R^2)\\frac{p-1}{n-p}\\end{equation}\\] 2.6 Analyse der Residuen und Überprüfung der Modellannahmen Die Residuen \\(r_i = y_i - \\hat{y}_i\\) dienen als Annäherungen an die unbekannten Resteffekte \\(\\epsilon_i\\) und zur Überprüfung der Modellannahmen. 2.6.1 Tukey-Anscombe Plot Der Tukey-Anscombe Plot ist ein graphisches Tool zur Feststellung von Abhängigkeiten zwischen den Residuen \\(r_i\\) und den gefitteten Werten \\(\\hat{y}_i\\). Im Tukey-Anscombe Plot werden auf der x-Achse die gefitteten Werte und auf der y-Achse die Resiuden aufgetragen. Idealerweise sind die Resiuden zufällig verteilt und zeigen kein Muster. In R erzeugt man den Tukey-Anscombe Plot über die Hilfsfunktionen fitted() und residuals(). Die Ergebnisse der beiden Funktionen werden einfach an die plot()-Funktion übergeben. data(&quot;anscombe&quot;) fit.lm.ta &lt;- lm(y1 ~ ., data = anscombe) plot(fitted(fit.lm.ta), residuals(fit.lm.ta)) Der obige Plot zeigt eine ideale Situation, wo keine systematischen Muster zu erkennen sind. Die folgenden vier Plots sind (Bühlmann and Mächler 2014) entnommen und zeigen Probleme bei der Anpassung von linearen Modellen auf. 2.6.2 Der QQ-Plot Annahmen zur Verteilung der zufälligen Grössen im linearen Modell können mit dem sogenannten QQ-Plot überprüft werden. Die Abkürzung “QQ” steht hier für Quantil-Quantil und meint, dass wir die empirischen Quantile den theoretischen Quantilen einer bestimmten Verteilung gegenüberstellen. Im Fall, dass wir gegen die theoretischen Quantile einer Normalverteilung testen, heisst der QQ-Plot auch Normal Plot. In R können wir den QQ-Plot für die Residuen, wie folgt erzeugen. qqnorm(residuals(fit.lm.ta)) qqline(residuals(fit.lm.ta)) Stimmen die Quantile der empirische Verteilung der Residuen gut mit den theoretischen Quantilen überein, dann liegen die Punkte im QQ-Plot auf einer Geraden. Falls die empirische Verteilung bedeutende Abweichungen zeigt von der angenommenen Verteilung, so zeigt der Verlauf der Punkte systematische Abweichungen, wie das in den folgenden Graphiken der Fall ist. 2.7 Selektion eines Modells Gegeben sei das lineare Modell \\[\\begin{equation}y_i = \\sum_{i=1}^n \\beta_j x_{ij} + \\epsilon_i \\quad (i = 1,\\ldots,n)\\end{equation}\\] mit \\(\\epsilon_1,\\ldots,\\epsilon_n \\text{ i.i.d. }\\), \\(E\\left[\\epsilon_i \\right] = 0\\) und \\(Var(\\epsilon_i) = \\sigma^2\\). Bis anhin hatten wir immer alle erklärenden Variablen \\(x_1,\\ldots,x_p\\) im Modell berücksichtigt. Wir können uns aber auch fragen, ob dies Sinn macht, wenn gewisse erklärende Variablen nicht relevant sind für die Modellierung der Zielgrössen. Hinzu kommt noch, dass für jede erklärende Variablen ein unbekannter Parameter \\(\\beta_j\\) geschätzt werden muss. Jeder geschätzte Parameter trägt zur Variabilität der gefitteten Werte bei, ob die erklärende Variable relevant ist oder nicht. Somit wird oft nach dem optimalen Modell und nicht nach dem wahren Modell gesucht. Das optimale Modell ist definiert als das Modell mit dem minimalen Subset an erklärenden Variablen, welche alle relevant sind für die Modellierung der Zielgrösse. Formell können wir das soeben Erklärte wie folgt zusammenfassen. Angenommen, wir wollen die folgende Vorhersage, nennen wir sie \\(\\mathcal{M}\\), optimieren. \\[\\begin{equation}\\sum_{r=1}^q \\hat{\\beta}_{j_r}x_{ij_r}\\end{equation}\\] welche die \\(q\\) erklärenden Variablen mit den Indices \\(j_1,\\ldots,j_q \\in \\{1,\\ldots,p\\}\\) enthält. Wir brauchen ein Entscheidungskriterium um die Vorhersage \\(\\mathcal{M}\\) mit den \\(q\\) Parametern mit dem vollen Modell, welches alle erklärenden Variablen enthält vergleichen zu können. 2.7.1 Mallows \\(C_p\\)-Statistik Die Summenquadrate der Residuen \\(SSE(\\mathcal{M})\\) für die Vorhersage \\(\\mathcal{M}\\) können wir nicht als Kriterium verwenden, denn \\(SSE(\\mathcal{M})\\) nimmt ab mit zunehmener Anzahl \\(q\\) an Parametern. Der mittlere quadrierte Fehler bei der Verwendung von \\(\\mathcal{M}\\) anstelle vom vollen Modell, kann als \\[\\begin{equation}n^{-1} SSE(\\mathcal{M}) - \\hat{\\sigma}^2 + 2\\hat{\\sigma}^2q/n\\end{equation}\\] geschätzt werden, wobei \\(\\hat{\\sigma}^2\\) der geschätzte Restvarianz aus dem vollen Modell entspricht. Da \\(n\\) und \\(\\hat{\\sigma}^2\\) für alle Submodelle \\(\\mathcal{M}\\) konstant sind, können wir als Kriterium für den Modellvergleich die Statistik \\[\\begin{equation}C_p(\\mathcal{M}) = \\frac{SSE(\\mathcal{M})}{\\hat{\\sigma}^2} - n + 2q\\end{equation}\\] verwenden. 2.7.2 Modellwahl mit dem \\(C_p\\)-Kriterium Für das volle Modell mit \\(p\\) erklärenden Variablen gibt es \\(2^p-1\\) Submodelle oder Vorhersagen \\(\\mathcal{M}\\). Somit ist ein Vergleich der \\(C_p\\)-Statistik aller Submodelle nur machbar, wenn \\(p\\) nicht zu gross, d.h. kleiner als \\(16\\) ist. Für \\(p \\ge 16\\) werden die folgenden zwei schrittweisen Algorithmen vorgeschlagen. 2.7.2.1 Vorwärts-Integration (Forward Selection) Starte mit dem minimalen Modell \\(\\mathcal{M}_0\\), welches nur ein globales Mittel enthält Wähle die erklärende Variable, welche die Summe der quadrierten Residuen am meisten reduziert und nimm diese ins Modell auf Wiederhole Schritt 2 bis alle erklärenden Variablen im Modell aufgenommen wurden. Das produziert eine Sequenz von Submodellen \\(\\mathcal{M}_0, \\mathcal{M}_1, \\mathcal{M}_2, \\ldots\\). Wähle aus der Sequenz der Submodelle dasjenige mit dem kleinsten Wert der \\(C_p\\)-Statistik 2.7.2.2 Rückwärts-Elimination (Backward Selection) Wir starten mit dem vollen Modell, welches alle erklärenden Variablen enthält Entferne die erklärende Variable vom vollen Modell, welche die Summe der quadrierten Residuen am wenigsten reduziert. Wiederhole Schritt 2 bis alle erklärenden Variablen entfernt wurden. Das führt zu einer Sequenz von Submodellen. Wähle dasjenige Submodell aus der Sequenz an Submodellen mit minimaler \\(C_p\\)-Statistik 2.7.3 Bemerkungen Rückwärts-Elimination von erklärenden Variablen funktioniert im allgemeinen besser als Vorwärts-Integration, aber ist auch teuerer im Bezug auf Rechenleistung. In (Bühlmann and Mächler 2014) wird die Vorwärts-Integration für den Fall dass \\(p \\ge n\\) als taugliche Methode bezeichnet. Erfahrungen im Bereich der Effektschätzung in der genomischen Selektion haben aber gezeigt, dass Vorwärts-Integration zu keiner stabilen Prozedur für die Selektion eines guten Modells führt. Schon die Autoren in (Meuwissen, Hayes, and Goddard 2001) haben für simulierte Daten gezeigt, dass die Vorwärts-Integration von SNP-Effekten als erklärende Variablen bei der Identifikation der wichtigen SNP-Effekte versagte. Offenbar gibt es bie einer sehr grossen Anzahl von erklärenden Variablen \\(p\\) im Vergleich zur Anzahl der verfügbaren Beobachtungen \\(n\\) das Problem, dass im Schritt 2 der Vorwärts-Integration viele erklärende Variablen die Summe der quadrierten Residuen um mehr oder weniger den gleich Betrag reduzieren. Dann haben wir das Problem, dass wir eine Auswahl zwischen fast gleichwertigen erklärenden Variablen treffen müssen. Diese Auswahl ist offensichtlich kritisch und kann zu sehr verschiedenen Endergebnissen in der Modellwahl führen. References "],
["gblup.html", "Kapitel 3 Genomic Best Linear Unbiased Prediction (GBLUP) 3.1 DNA Markers 3.2 Markerinformationen in BLUP-Verfahren 3.3 Genomische Verwandtschaftsmatrix (GRM) 3.4 Wie GBLUP funktioniert", " Kapitel 3 Genomic Best Linear Unbiased Prediction (GBLUP) Die Methode GBLUP verwendet genomische Verwandtschaftsbeziehungen zur Schätzung des genetischen Potentials von Individuen. Die genomische Verwandtschaft wird aufgrund von DNA-Informationen geschätzt. Die genomische Verwandtschaftsmatrix \\(G\\) definiert die Kovarianz zwischen Individuen aufgrund von Ähnlichkeiten auf dem Niveau der genomischen Information. Diese Definition der Kovarianz steht im Gegensatz zur Kovarianz aufgrund der erwarteten Ähnlichkeit aufgrund von Pedigrees in der traditionellen Verwandtschaftsmatrix \\(A\\). 3.1 DNA Markers Die Verfügbarkeit von Markerkarten in genügender Dichte verteilt über das ganze Genom, ermöglicht die Schätzung des genetischen Potentials von Individuen basierend auf der beobachteten Markerinformation. Das aktuell am weitesten verbreitete Markermodell wird als Single Nucleotide Polymorphism (SNP) bezeichnet. Bei den SNPs handelt es sich um Positionen im Genom, an welchen Unterschiede zwischen Individuen einer Population bei einer einzelnen DNA-Base auftreten. Nehmen wir an, dass Genorte (QTL), welche wichtig sind für die Expression von interessanten phänotypischen Eigenschaften, sich in der Nähe von beobachteten SNPs befinden, können wir aufgrund von Kopplungsungleichgewichten zwischen SNPs und QTL eine statistische Beziehung zwischen phänotypischen Werten und SNP-Genotypen modellieren. Für die Schätzung der SNP-Effekte auf phänotypische Ausprägungen stehen verschiedene statistische Verfahren zur Verfügung. Als einfachste Methode können wir die Regression auf einzelne SNP-Marker bezeichnen. Das Problem der Single-Marker Regression liegt in der im Vergleich zur Anzahl der Beobachtungen sehr grossen Anzahl an SNP-Markern. Eine Lösung dieses Problems besteht darin, dass die SNP-Effekte im linearen Modell als zufällige Effekte aufgefasst werden. Dadurch wird das einfache lineare Regressionsmodell in ein lineares gemischtes Modell verwandelt. Aus der traditionellen Zuchtwertschätzung ((Hofer 1990) und (von Rohr 2016)) kennen wir die Mischmodellgleichungen, welche uns Schätzwerte für fixe und zufällige Effekte mit BLUE- und BLUP-Eigenschaften liefern. Bei diesem Verfahren werden aber allen SNPs der gleiche Varianzanteil zugesprochen. Die totale genetische Varianz wird somit auf alle SNPs gleichmässig aufgeteilt. Bayes’sche Methoden, wie sie in Kapitel 5 beschrieben sind, erlauben es verschiedenen genomischen Regionen verschiedene Varianzanteile zuzuordnen. 3.2 Markerinformationen in BLUP-Verfahren Parameterschätzverfahren, welche zu Schätzungen für fixe Effekte mit Best Linear Unbiased Estimation (BLUE)-Eigenschaften und zu Vorhersagen von zufälligen Effekten mit Best Linear Unbiased Prediction (BLUP)-Eigenschaften führen werden hier kurz als BLUP-Verfahren bezeichnet. Die traditionelle Zuchtwertschätzung basiert auf Pedigreeinformationen zur Schätzung der Covarianz zwischen zufälligen Effekten (Zuchtwerten) von verwandten Tieren. Die Covarianz kann aber auch aufgrund von Markerinformation geschätzt werden. Die aufgrund von SNPs erstellte Matrix wird als Genomic Relationship Matrix (GRM) bezeichnet. Wir besprechen hier zwei verschiedene Ansätze Ridge Regression (RR)-BLUP Genomic BLUP (GBLUP) 3.2.1 Ridge Regression (RR) BLUP Diese Methode wurde von (Meuwissen, Hayes, and Goddard 2001) und (Habier, Fernando, and Dekkers 2007) untersucht. Dabei haben die Autoren das folgende Modell angenommen. \\[\\begin{equation} y = 1_n\\mu + Wq + e \\label{eq:RRBlupModel} \\end{equation}\\] Die Genotypen an jedem SNP-Locus in Matrix \\(W\\) werden mit \\(0\\), \\(1\\) und \\(2\\) codiert. Diese Codes repräsentieren die Anzahl an SNP-Allelen mit positiver Wirkung. Die SNP-Effekte werden als zufällig betrachtet. Das Modell in () ist also ein gemischtes lineares Modell. Die unbekannten Parameter werden mit Hilfe von Mischmodellgleichungen geschätzt. Der geschätzte genomische Zuchtwert entspricht der Summe der SNP-Effekte über alle SNP-Loci. Die durch die SNP-Effekte erklärte Varianz ist gleich \\(WW^T\\sigma_q^2\\). Die Varianz der Resteffekte wird als \\(I\\sigma_e^2\\) angenommen. Somit entspricht die Covarianz zwischen Beobachtungen \\(WW^T\\sigma_q^2 + I\\sigma_e^2\\). In RR-BLUP wird die Varianz an allen SNPs als konstant angenommen. 3.2.2 Genomic BLUP (GBLUP) Die zweite Methode, welche genomische Information berücksichtigt verwendet die genomische Verwandtschaftsmatrix \\(G\\) anstelle der additiv genetischen Verwandtschaftsmatrix \\(A\\) in einem gemischten linearen Modell. Dieser Ansatz wird als gBLUP bezeichnet. Das Modell zur Umsetzung von gBLUP lautet \\[\\begin{equation} y = Xb + Zg + e \\label{eq:GblupModel} \\end{equation}\\] Der Vektor \\(g\\) enthält zufällige genetische Effekte für alle Tiere, welche typisiert sind. Diese Tiere können Beobachtungen aufweisen oder nicht. Die Tiere mit Beobachtungen und Typisierungsergebnisse werden allgemein als Trainings- oder Referenzpopulation bezeichnet. Die Tiere ohne phänotypische Beobachtung mit Typisierungsergebnissen bilden das Testset, für welche die genomischen Zuchtwerte geschätzt werden sollen. Die Varianz \\(var(g) = G * \\sigma_g^2\\) wobei \\(G\\) der genomischen Verwandtschaftsmatrix entspricht. Die zufälligen Resteffekte werden als unabhängig angenommen mit der Covarianzmatrix \\(var(e) = I*\\sigma_e^2\\). GBLUP hat drei wichtige Vorteile im Vergleich zu RR-BLUP. Die Dimension der genetischen Effekte in GBLUP beträgt \\(n\\times n\\), wobei \\(n\\) die Anzahl Tiere sind. In RR-BLUP beträgt diese Dimension \\(m\\times m\\), wobei \\(m\\) der Anzahl an SNP-Markern entspricht. Somit ist GBLUP effizienter im Hinblick auf Rechenresourcen. Die Genauigkeiten der genomischen Zuchtwerte können bei GBLUP analog zu den Genauigkeiten der Zuchtwerte im BLUP-Tiermodell berechnet werden. GBLUP kann mit Pedigree-basierten Informationen zu den sogenannten single step Verfahren kombiniert werden (Misztal, Legarra, and Aguilar 2009). 3.3 Genomische Verwandtschaftsmatrix (GRM) Die Covarianz zwischen den gentischen Effekten \\(g\\) im Modell () wird über die genomische Verwandtschaftsmatrix \\(G\\) ausgedrückt. Analog zum BLUP-Tiermodell, soll die Covarianz der genetischen Effekte als Produkt der genomischen Verwandtschaftsmatrix \\(G\\) mal die Varianzkomponente \\(\\sigma_g^2\\) dargestellt werden. 3.3.1 Herleitung der GRM Als erstes stellt sich die Frage, wie wir die genetischen Effekte \\(g\\) überhaupt definieren sollen. Auf dieser Definition von \\(g\\) aufbauend können wir uns anschliessend überlegen, wie die Matrix \\(G\\) aufgestellt werden kann. Die folgenden Eigenschaften für die genetischen Effekte \\(g\\) und für die GRM \\(G\\) sollen gelten. Die genetischen Effekte \\(g\\) sollen der Summe aller SNP-Effekte \\(q\\) entsprechen. Die genetischen Effekte \\(g\\) sollen nicht als Absolute Werte sondern analog zu den Zuchtwerten aus dem BLUP-Tiermodell als Abweichungen von einer festgelegten Basis definiert sein, das heisst der Erwartungswert \\(E\\left[g\\right] = 0\\). Wie schon erwähnt soll die Covarianz der genetischen Effekte \\(g\\) dem Produkt aus GRM \\(G\\) und der Varianzkomponente \\(\\sigma_g^2\\) entsprechen, d.h. \\(var(g) = G * \\sigma_g^2\\). Die GRM \\(G\\) soll ähnlich wie die additive Verwandtschaftsmatrix \\(A\\) aussehen, d.h. die Diagnoalelemente sollen um \\(1\\) liegen und auf der Offdiagonalen sollen hohe Werte mit genetisch ähnlichen Tieren assoziiert werden. Als Informationsquellen für die Definition von \\(g\\) und zum Aufstellen der GRM \\(G\\) haben wir die SNP-Markerinformationen zur Verfügung. Das hier vorgestellte Material basiert auf den Arbeiten von (VanRaden 2008) und von (Gianola et al. 2009). In den folgenden Unterabschnitten wollen wir die Konsequenzen der oben aufgelisteten Eigenschaften analysieren und daraus die GRM \\(G\\) aufstellen. 3.3.2 Genetische Effekte als Summe der SNP-Effekte Basierend auf der SNP-Markerinformation können wir Effekte für die einzelnen SNP-Marker schätzen. Wir nehmen hier also an, dass wir den Vektor \\(q\\) kennen. Formell bedeutet die Eigenschaft, dass die genetischen Effekte \\(g\\) als Summe der SNP-Effekte \\(q\\) dargestellte werden können, dass es eine Matrix \\(U\\) gibt, für welche gilt, dass \\[\\begin{equation} g = U * q \\label{eq:VecGSumOfVecQ} \\end{equation}\\] wobei an dieser Stelle die Matrix \\(U\\) noch unbekannt ist. Wir werden diese anhand der nächsten Eigenschaft bestimmten. 3.3.3 Genetische Effekte als Abweichungen Die genetischen Effekte sollen, analog zu den Zuchtwerten aus dem BLUP-Tiermodell als Abweichungen von einer festgelegten Basis definiert werden. Die Basis stellt den Nullpunkt der genetischen Effekte dar. Somit kommen die einzelnen Genetischen Effekte \\(g_i\\) für Tier \\(i\\) aus einer Verteilung mit Erwartungswert \\(E\\left[g_i\\right] = 0\\). Die Frage ist nun, wie muss die Matrix \\(U\\) aussehen, dass unabhängig vom Vektor \\(q\\) der Erwartungswert der Komponenten von \\(g_i\\) gleich null ist. Betrachten wir die Zufallsvariable \\(w\\) mit den SNP-Genotypencodes in Matrix \\(W\\) im RR-BLUP-Modell () und nehmen wir an, das Hardy-Weinberg-Gleichgewicht gelte für alle SNP-Genorte, dann sind folgende Realisierungswerte für \\(w\\) möglich \\[\\begin{equation} w = \\left\\{ \\begin{array}{lll} 0 &amp; \\text{mit Wahrscheinlichkeit} &amp; (1-p)^2\\\\ 1 &amp; \\text{mit Wahrscheinlichkeit} &amp; 2p(1-p) \\\\ 2 &amp; \\text{mit Wahrscheinlichkeit} &amp; p^2 \\end{array} \\right. \\label{eq:RandVarGenotypesW} \\end{equation}\\] Der Erwartungswert der Zufallsvariablen \\(w\\) entspricht \\[\\begin{equation} E\\left[w\\right] = 0*(1-p)^2 + 1 * 2p(1-p) + 2 * p^2 = 2p \\label{eq:ExpectedValueW} \\end{equation}\\] Die Matrix \\(U\\) aus () berechnen wir als Differenz zwischen den Matrizen \\(W\\) und \\(P\\). Die Matrix \\(P\\) besteht aus Kolonnenvektoren \\(1_n2p_j\\) der Länge \\(n\\) für die entsprechenden SNPs \\(j = 1,\\ldots ,m\\). Für einen bestimmten SNP-Locus \\(j\\) folgen aufgrund der Definition der Matrix \\(U\\) die folgenden Codierungen der SNP-Genotypen. Dabei nehmen wir an, dass das Allel \\((G_{2})_j\\) einerseits mit Frequenz \\(p_j\\) auftritt und andererseits dasjenige Allel mit der positiven Wirkung ist. Die nachfolgende Tabelle gibt eine Übersicht über die genotypischen Werte und der Codierung der Genotypen in der Matrix \\(U\\) für die drei auftretenden Genotypen am SNP-Locus \\(j\\). Unter der Annahme des Hardy-Weinberg-Gleichgewichts für die Genotypen am SNP-Locus \\(j\\) können wir den Erwartungswert für den genetischen Effekt \\(g\\) am SNP-Locus \\(j\\) berechnen. Der Erwartungswert wird über alle Komponenten des Vektors \\(g\\) der genetischen Effekte berechnet. \\[\\begin{equation} \\begin{split} E\\left[g\\right]_j &amp; = \\left[(1-p_j)^2(-2p_j) + 2p_j(1-p_j)(1-2p_j) + p_j^2(2-2p_j)\\right]q_j\\\\ &amp; = \\left[(1 - 2p_j + p_j^2)(-2p_j) + (2p_j - 2p_j^2)(1-2p_j) + p_j^2(2-2p_j)\\right]q_j\\\\ &amp; = \\left[-2p_j + 4p_j^2 - 2p_j^3 + 2p_j - 4p_j^2 - 2p_j^2 + 4p_j^3 + 2p_j^2 - 2p_j^3\\right]q_j\\\\ &amp; = 0 \\end{split} \\label{eq:ExpectedValueGj} \\end{equation}\\] Die gleiche Herleitung lässt sich für jeden SNP Locus \\(j = 1,\\ldots, m\\) machen. Somit ist der Erwartungswert über alle genetischen Komponenten \\(g_i\\) gleich null, unabhängig von den SNP-Effekten \\(q_j\\) am Locus \\(j\\). Somit haben wir die Matrix \\(U\\), welche aus den SNP-Effekten \\(q\\) in () die genetischen Effekte \\(g\\) in () berechnet, gefunden. 3.3.4 Alternative Codierung Anstelle der verwendeten Codierungen von \\(0\\), \\(1\\) und \\(2\\) für die SNP-Genotypen können diese auch mit \\(-1\\), \\(0\\) und \\(1\\) codiert werden. Diese Codierung wird in (VanRaden 2008) zum Aufstellen der GRM \\(G\\) verwendet. Auch die Autoren des Papers (Gianola et al. 2009) verwenden diese alternative Codierung. Am prinzipiellen Vorgehen ändert sich aber dadurch nichts. Nur die verwendeten Werte in den Matrizen sind anders. Die Matrix \\(W\\) unter der alternativen Codierung besteht aus Werten \\(-1\\), \\(0\\) und \\(1\\). Die Zufallsvariable \\(w\\), welche wir in () definiert und zur Bestimmung der Matrix \\(P\\) verwendet hatten, sieht unter der alternativen Codierung wie folgt aus. \\[\\begin{equation} w = \\left\\{ \\begin{array}{lll} -1 &amp; \\text{mit Wahrscheinlichkeit} &amp; (1-p)^2\\\\ 0 &amp; \\text{mit Wahrscheinlichkeit} &amp; 2p(1-p) \\\\ 1 &amp; \\text{mit Wahrscheinlichkeit} &amp; p^2 \\end{array} \\right. \\label{eq:RandVarGenotypesWalternative} \\end{equation}\\] Der Erwartungswert für \\(w\\) unter der alternativen Codierung ist dann \\[\\begin{equation} E\\left[w\\right] = (-1)*(1-p)^2 + 0 * 2p(1-p) + 1 * p^2 = -1 + 2p - p^2 + p^2 = 2p - 1 = 2(p - 0.5) \\label{eq:ExpectedValueWalternative} \\end{equation}\\] Die Kolonne \\(j\\) der Matrix \\(P\\) besteht aus dem Kolonnenvektoren \\(1_n*2(p_j - 0.5)\\). Die Matrix \\(U\\) (welche der Matrix \\(Z\\) in (VanRaden 2008) entspricht) kann wird berechnet als \\[\\begin{equation} U = W - P \\label{eq:MatrixUalternative} \\end{equation}\\] Die Elemente der Matrix \\(U\\) der Genotypen am SNP-Locus \\(j\\) unter der alternativen Codierung lauten Der Erwartungswert der Komponenten der genetischen Effekte \\(g\\) aufgrund des SNP-Locus \\(j\\) beträgt dann \\[\\begin{equation} \\begin{split} E\\left[g\\right]_j &amp; = \\left[(1-p_j)^2(-1-2(p_j-0.5)) + 2p_j(1-p_j)(-2(p_j-0.5)) + p_j^2(1-2(p_j-0.5))\\right]q_j\\\\ &amp; = \\left[- 2 \\ p_j^{3} + 4 \\ p_j^{2} - 2 \\ p_j + 4 \\ p_j^{3} - 6 \\ p_j^{2} + 2 \\ p_j - 2 \\ p_j^{3} + 2 \\ p_j^{2} \\right]q_j\\\\ &amp; = 0 \\end{split} \\label{eq:ExpectedValueGjalternative} \\end{equation}\\] 3.3.5 Varianz der genetischen Effekte Aufgrund der postulierten Eigenschaften soll die Varianz \\(var(g)\\) der genetischen Effekte berechnet werden als das Produkt der GRM \\(G\\) mal die gemeinsame Varianzkomponente \\(\\sigma_g^2\\). Als Formel geschrieben lautet der Ausdruck für \\(var(g)\\) \\[\\begin{equation} var(g) = G * \\sigma_g^2\\text{.} \\label{eq:VarGFormula} \\end{equation}\\] Aus der Theorie folgt unter Berücksichtigung von (), dass \\[var(g) = U * var(q) * U^T\\] ist. Für die Varianz der SNP-Effekte nehmen wir an, dass \\(var(q) = I\\sigma_q^2\\). Dies bedeutet, dass sich der Ausdruck für die Kovarianz der genetischen Effekte \\(g\\) vereinfacht zu \\(var(g) = UU^T \\sigma_q^2\\). Im Paper von (Gianola et al. 2009) wurde \\(\\sigma_g^2\\) aus \\(\\sigma_q^2\\) hergeleitet. Das Resultat dieser Herleitung lautet \\[\\begin{equation} \\sigma_g^2 = 2 \\sum_j^m p_j(1-p_j) \\sigma_q^2 \\label{eq:GenomicVariance} \\end{equation}\\] Fassen wir alle diese Beziehungen zusammen, können wir die verschiedenen Ausdrücke für die Kovarianz der genetischen Effekte gleichsetzen. \\[\\begin{equation} var(g) = G * \\sigma_g^2 = UU^T \\sigma_q^2 \\label{eq:GenomicVarianceTwoExpr} \\end{equation}\\] Ersetzen wir \\(\\sigma_g^2\\) in () durch den Ausdruck in () so folgt \\[\\begin{equation} G * 2 \\sum_j^m p_j(1-p_j) \\sigma_q^2 = UU^T \\sigma_q^2 \\label{eq:GenomicVarianceInserted} \\end{equation}\\] Aus der Gleichung in () kann die genomische Verwandtschaftsmatrix \\(G\\) berechnet werden als \\[\\begin{equation} G = \\frac{UU^T}{2\\sum_j p_j(1-p_j)} \\label{eq:GenomRelMat} \\end{equation}\\] 3.3.6 R-Code zur Berechnung der GRM Im Kapitel 13 von (Clark and van der Werf 2013) ist ein R-Programm zur Berechnung der genomischen Verwandtschaftsmatrix aufgeführt. Wir wollen dieses Programm analysieren und präsentieren auch eine alternative Art der Berechnung der genomischen Verwandtschaftsmatrix. # Making the genomic relationship matrix nmarkers &lt;- 1000 # Reading SNP-Genotype codes (0,1,2) from file data &lt;- matrix(scan(&quot;genotypes.txt&quot;),ncol = nmarkers,byrow = TRUE) # Initialisation of variables sumpq = 0 freq = dim(data)[1] P = freq lamda = ncol(data) # Compute allele frequencies and column vectors of matrix P for(i in 1:ncol(data)){ (freq[i] &lt;- ((mean(data[,i])/2))) (P[i] = (2*(freq[i]-0.5))) (sumpq = sumpq+(freq[i]*(1-freq[i]))) } # Transform coding from (0,1,2) to (-1,0,1) and subtract P Z &lt;- data for(i in 1:nrow(data)){ for(j in 1:ncol(data)){ (Z[i,j] &lt;- ((data[i,j]-1)-(P[j]))) } } # Compute GRM Zt = t(Z) ZtZ = Z%*%Zt G = ZtZ/(2*sumpq) G ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.8978768 -0.2330233 -0.2286366 -0.2067029 -0.2295139 ## [2,] -0.2330233 0.8469907 -0.2014388 -0.2189858 -0.1935427 ## [3,] -0.2286366 -0.2014388 0.9171785 -0.2453062 -0.2417968 ## [4,] -0.2067029 -0.2189858 -0.2453062 0.9215652 -0.2505703 ## [5,] -0.2295139 -0.1935427 -0.2417968 -0.2505703 0.9154238 Das gezeigte Programm zur Berechnung der genomischen Verwandtschaftsmatrix aus (Clark and van der Werf 2013) kann vereinfacht werden. Insbesondere können die for-loops durch einfachere Funktionen und vektorisierte Berechnungen ersetzt werden. Das folgende Programm berechnet die gleiche genomische Verwandtschafsmatrix. # Making the genomic relationship matrix nmarkers &lt;- 1000 # Reading SNP-Genotype codes (0,1,2) from file data &lt;- matrix(scan(&quot;genotypes.txt&quot;),ncol = nmarkers,byrow = TRUE) # Compute allele frequencies, column vectors of matrix P and sum of frequency products freq &lt;- apply(data, 2, mean) / 2 P &lt;- 2 * (freq - 0.5) sumpq &lt;- sum(freq*(1-freq)) # Compute matrix Z by first changing the coding from (0,1,2) # to (-1,0,1) and then subtracting matrix P Z &lt;- data - 1 - matrix(P, nrow = nrow(data), ncol = ncol(data), byrow = TRUE) # Z%*%Zt is replaced by tcrossprod(Z) (matGrm &lt;- tcrossprod(Z)/(2*sumpq)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.8978768 -0.2330233 -0.2286366 -0.2067029 -0.2295139 ## [2,] -0.2330233 0.8469907 -0.2014388 -0.2189858 -0.1935427 ## [3,] -0.2286366 -0.2014388 0.9171785 -0.2453062 -0.2417968 ## [4,] -0.2067029 -0.2189858 -0.2453062 0.9215652 -0.2505703 ## [5,] -0.2295139 -0.1935427 -0.2417968 -0.2505703 0.9154238 Wir können überprüfen, ob die beiden Matrizen identisch sind all.equal(G, matGrm) ## [1] TRUE Das oben gezeigte alternative Programm zur Berechnung der genomischen Verwandtschaftsmatrix kann so verwendet werden. Da R eine funktionale Programmiersprache ist, soll es unser Ziel sein, möglichst viele Aufgaben in Funktionen zu kapseln. Die folgende Funktion computeMatGrm() berechnet aufgrund der Datenmatrix die genomischen Verwandtschaftsmatrix auf. #&#39; Compute genomic relationship matrix based on data matrix computeMatGrm &lt;- function(pmatData) { # Allele frequencies, column vector of P and sum of frequency products freq &lt;- apply(pmatData, 2, mean) / 2 P &lt;- 2 * (freq - 0.5) sumpq &lt;- sum(freq*(1-freq)) # Changing the coding from (0,1,2) to (-1,0,1) and subtract matrix P Z &lt;- data - 1 - matrix(P, nrow = nrow(pmatData), ncol = ncol(pmatData), byrow = TRUE) # Z%*%Zt is replaced by tcrossprod(Z) return(tcrossprod(Z)/(2*sumpq)) } # Computing the genomic relationship matrix using the above defined function nmarkers &lt;- 1000 # Reading SNP-Genotype codes (0,1,2) from file data &lt;- matrix(scan(&quot;genotypes.txt&quot;),ncol = nmarkers,byrow = TRUE) # calling the function (matGrmFunc &lt;- computeMatGrm(pmatData = data)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.8978768 -0.2330233 -0.2286366 -0.2067029 -0.2295139 ## [2,] -0.2330233 0.8469907 -0.2014388 -0.2189858 -0.1935427 ## [3,] -0.2286366 -0.2014388 0.9171785 -0.2453062 -0.2417968 ## [4,] -0.2067029 -0.2189858 -0.2453062 0.9215652 -0.2505703 ## [5,] -0.2295139 -0.1935427 -0.2417968 -0.2505703 0.9154238 # checking the result all.equal(G, matGrmFunc) ## [1] TRUE 3.4 Wie GBLUP funktioniert Die genomischen Verwandtschaftsmatrix erlaubt es uns die Daten der Tiere mit Beobachtung mit den Tieren ohne Beobachtungen aber mit SNP-Genotypen zu verlinken. Die Inverse \\(G^{-1}\\) der genomischen Verwandtschaftsmatrix wird zur Modellierung der Kovarianzstruktur zwischen den genetischen Effekten und somit zwischen den genomischen Zuchtwerten verwendet. Wie schon beim BLUP-Tiermodell werden auch bei GBLUP die unbekannten Effekte mit Mischmodellgleichungen geschätzt. Für das GBLUP-Modell in () lauten die Mischmodellgleichungen, wie folgt. \\[\\left[ \\begin{array}{lll} X^TX &amp; X^TZ &amp; 0 \\\\ Z^TX &amp; Z^TZ + G^{11} &amp; G^{12} \\\\ 0 &amp; G^{21} &amp; G^{22} \\\\ \\end{array}\\right] \\left[ \\begin{array}{l} \\hat{b} \\\\ \\hat{g}_1 \\\\ \\hat{g}_2 \\\\ \\end{array}\\right] = \\left[ \\begin{array}{l} X^Ty \\\\ Z^Ty \\\\ 0 \\\\ \\end{array}\\right] \\] \\(G^{11}\\) steht für den Teil der Inversen \\(G^{-1}\\), der zu den Tieren mit phänotypischen Beobachtungen gehört. Analog dazu entspricht \\(G^{22}\\) dem Teil von \\(G^{-1}\\), der die Tiere ohne Beobachtungen mit nur SNP-Genotypen beinhaltet. Die Teile \\(G^{12}\\) und \\(G^{21}\\) verknüpfen die Tiere mit und ohne Beobachtungen. Bei den oben gezeigten Mischmodellgleichungen wird ein Verhältnis der Varianzkomponenten \\(\\sigma_g^2\\) und \\(\\sigma_e^2\\) von \\(1\\) angenommen. Aufgrund der letzten Zeile der GBLUP-Mischmodellgleichungen ist ersichtlich, dass die genomischen Zuchtwerte \\(\\hat{g}_2\\) der Tiere ohne phänotypische Beobachtungen aufgrund der Schätzungen \\(\\hat{g}_1\\) der genomischen Zuchtwerte der Tiere mit Beobachtungen vorhergesagt werden können. \\[\\begin{equation} \\hat{g}_2 = -\\left( G^{22}\\right)^{-1}G^{21}\\hat{g}_1 \\label{eq:GenomicBvAnimalNoPhen} \\end{equation}\\] Die Beziehung in () wird auch als genomische Regression der Zuchtwerte der Tiere ohne Beobachtung auf die genomischen Zuchtwerte der Tiere mit Beobachtung bezeichnet. References "],
["chpt-lasso.html", "Kapitel 4 Least Absolute Shrinkage And Selection Operator (LASSO) 4.1 Stochastische Restkomponente 4.2 Parameterschätzung 4.3 Alternativen zu Least Squares 4.4 Lasso 4.5 Bestimmung von \\(\\lambda\\) 4.6 Analyse mit LASSO in R", " Kapitel 4 Least Absolute Shrinkage And Selection Operator (LASSO) Das lineare Modell \\[\\begin{equation} y_i = \\beta_0 + \\sum_{j=1}^p \\beta_jx_{ij} + \\epsilon_i \\label{eq:StandardLinMod} \\end{equation}\\] für eine Beobachtung \\(i\\) (\\(i=1,\\ldots,n\\)) wird zur Modellierung von Zusammenhängen zwischen den erklärenden Variablen \\(x_{i1},\\ldots,x_{ip}\\) und der Zielgrösse \\(y_i\\) verwendet. In einem Regressionsmodell werden die unbekannten Parameter \\(\\beta_j \\ (j=0,\\ldots,p)\\) mit Least Squares geschätzt. Die \\((p+1)\\) Werte \\(\\beta_0, ..., \\beta_p\\) und die Resteffekte \\(\\epsilon_i\\) sind unbekannt. Es wird angenommen, dass die Werte der erklärenden Variablen (\\(x_{i1}, x_{i2}, ..., x_{ip}\\)) exakt, d.h. ohne Messfehler oder andere Ungenauigkeiten, bekannt sind. Für einen Datensatz mit \\(n\\) Beobachtungen werden die resultierenden \\(n\\) Gleichungen vorzugsweise in Matrix-Vektor-Schreibweise notiert. \\[\\begin{equation} y = X\\beta + \\epsilon \\label{eq:StandardLinearModelMatrixVektor} \\end{equation}\\] 4.1 Stochastische Restkomponente Die \\(n\\) unbekannten Resteffekte im Vektor \\(\\epsilon\\) werden als zufällige Effekte modelliert, wobei angenommen wird, dass sich diese Resteffekte im Mittel aufheben, d.h., dass deren Erwartungswert \\(E(\\epsilon) = 0\\) ist. Die Streuung der Resteffekte wird im Standardmodell als konstant angenommen. Für die Covarianz des Vektors der Resteffekte bedeutet das, dass \\(var(\\epsilon) = I*\\sigma^2\\) ist. Die Varianzkomponente \\(\\sigma^2\\) ist neben den Koeffizienten im Vektor \\(\\beta\\) ein weiterer unbekannter Parameter, welcher von den Daten geschätzt werden muss. 4.2 Parameterschätzung Unter der Annahme, dass die Matrix \\(X\\) vollen Kolonnenrang hat, d.h. die Anzahl Beobachtungen \\(n\\) grösser ist als die Anzahl Parameter (hier \\(p+1\\)) lassen sich die unbekannten Parameter \\(\\beta\\) mit Least Squares schätzen. Der Least Squares Schätzer \\(\\hat{\\beta}\\) für \\(\\beta\\) wird berechnet aus \\[\\begin{equation} \\hat{\\beta} = argmin_{\\beta}||y - X\\beta||^2 \\label{eq:LsEstimateBeta} \\end{equation}\\] wobei \\(||.||\\) für die Euklidsche Norm (Länge) im \\(n\\)-dimensionalen Raum steht. Wird das Minimierungsproblem in Gleichung () aufgelöst, dann resultiert der folgende Ausdruck für \\(\\hat{\\beta}\\) \\[\\begin{equation} \\hat{\\beta} = (X^TX)^{-1}X^Ty \\label{eq:LsEstimateBetaSol} \\end{equation}\\] Betrachten wir den Ausdruck in Gleichung () wird klar, weshalb die Matrix \\(X\\) vollen Kolonnenrang haben muss, da nur so die Inverse \\((X^TX)^{-1}\\) berechnet werden kann. 4.3 Alternativen zu Least Squares Das lineare Modell () erweist sich in der Praxis als sehr brauchbar. Mit der Least Squares-Technik besteht auch eine einfache und sehr gut etablierte Methode zur Parameterschätzung. In kürzerer Vergangenheit auch mit dem Aufkommen des Phänomes von “Big Data”, welches das systematische Sammeln von grossen Datenmengen ermöglicht, treten häufiger Probleme auf, bei welchen die im einleitenden Abschnitt aufgestellte Bedingung an Least Squres (\\(n &gt; p\\)) nicht zutrifft. Da wir die positiven Eigenschaften des linearen Modells gerne beibehalten möchten, wurde nach Alternativen zu Least Squres gesucht. Diese möglichen Alternativen können in drei Kategorien eingeteilt werden. Subset Selektion: Aus den \\(p\\) erklärenden Variablen wird ein Subset von “relevanten” Variablen ausgewählt. Alle anderen Variablen werden ignoriert. Die relevanten Variablen werden oft aufgrund der Signifikanz des geschätzten Regressionskoeffizienten \\(\\beta_j\\) identifiziert. Regularisierung (Shrinkage): Alle \\(p\\) erklärenden Variablen werden verwendet. Die geschätzten Regressionskoeffizienten werden durch bestimmte Techniken gegen den Nullpunkt “gedrückt”. Dieser Prozess wird als Schrumpfung (Shrinkage) bezeichnet. Die so erzeugte Reduktion der Variabilität der Schätzwerte wird als Regularisierung bezeichnet. Dimensionsreduktion: Die \\(p\\) erklärenden Variablen werden zu \\(m\\) Linearkombinationen reduziert. Diese Reduktion kann mit Techniken, wie Principal Components Analysis oder Faktoranalyse gemacht werden. 4.4 Lasso Es gibt Schätzverfahren, welche mehrere der oben genannten Alternativen zu Least Squares kombinieren. Ein Beispiel dafür ist LASSO. LASSO steht für Least Absolute Shrinkage and Selection Operation und kombiniert “Subset Selection” und Regularisierung. Die Regularisierung wird durch das Hinzufügen eines Terms zu den Rest-Summenquadraten (\\(RSS\\)), welche bei Least Squares minimiert werden. In Gleichung () haben wir gesehen, wie \\(RSS\\) verwendet werden zur Berechnung der Least Squares Schätzer \\[\\begin{eqnarray} \\hat{\\beta}_{LS} &amp; = &amp; argmin_{\\beta}||y - X\\beta||^2 \\nonumber \\\\ &amp; = &amp; argmin_{\\beta} \\left\\{\\sum_{i = 1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2\\right\\} \\nonumber \\\\ &amp; = &amp; argmin_{\\beta} RSS \\label{eq:LsEstimateBetaExpandRSS} \\end{eqnarray}\\] 4.4.1 Regularisierung bei LASSO Bei LASSO wird nun zu \\(RSS\\) ein sogenannter Strafterm (penalty term) hinzugefügt. Dieser Strafterm beträgt \\(\\lambda\\sum_{j=1}^p|\\beta_j|\\). Der Term wird deshalb als Strafterm bezeichnet, weil er mit steigender Summe der Absolutbeträge aller \\(\\beta_j\\) immer grösser wird. Diese führt zum gewünschten Effekt der Regularisierung. Das heisst durch das Hinzufügen dieses Strafterms werden die Absolutbeträge und somit die Variabilität der Koeffizientenschätzungen begrenzt, was der eigentliche Sinn und Zweck der Regularisierung ist. In Formeln ausgedrückt, lauten die geschätzten Regressionskoeffizienten für LASSO, wie folgt: \\[\\begin{eqnarray} \\hat{\\beta}_{LASSO} &amp; = &amp; argmin_{\\beta} \\left\\{\\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda\\sum_{j=1}^p|\\beta_j| \\right\\} \\nonumber \\\\ &amp; = &amp; argmin_{\\beta} \\left\\{RSS + \\lambda\\sum_{j=1}^p|\\beta_j|\\right\\} \\label{eq:LsEstimateBetaLASSO} \\end{eqnarray}\\] 4.4.2 Subset Selection bei LASSO Wie schon im vorangegangenen Abschnitt beschrieben, dient der Strafterm \\(\\lambda\\sum_{j=1}^p|\\beta_j|\\) zur Regularisierung der geschätzten Koeffizienten \\(\\beta_j\\) im linearen Modell. Der Strafterm spielt auch eine entscheidene Rolle bei der Subset Selection. Dadurch, dass der Strafterm die Absolutbeträge der Koeffizienten \\(\\beta_j\\) summiert, werden die Schätzungen von gewissen Koeffizienten explizit auf Null gesetzt. Weshalb dieser Effekt der Subset Selection bei LASSO eintritt kann mit folgender Abbildung (siehe nächste Seite) erklärt werden. In dieser Abbildung sind nur zwei erklärende Variablen gezeigt und somit ist \\(p=2\\). Die Koeffizienten zu den erklärenden Variablen werden in der Abbildung mit \\(b\\) und nicht mit \\(\\beta\\) bezeichnet. Unter der Annahme, dass wir unendlich viele Daten hätten, wäre der Schätzer der Koeffizienten \\(b_j\\) mit minimalem Fehler am Punkt, welcher in der Abbildung mit \\(\\hat{b}\\) bezeichnet ist. Die grünen Ellipsen um diesen Punkt \\(\\hat{b}\\) sind die Linien mit konstantem Fehler. Die rote Linie steht für die Grenze, welche durch den Strafterm aus LASSO entsteht. Das heisst geschätzte Koeffizienten können nur links dieser roten Linie liegen. Da wir den geschätzten Koeffizienten \\(\\hat{b}_j\\) einerseits minimalen Fehler erreichen wollen und auf der anderen Seite innerhalb der Regularisierungsgrenzen sein müssen, liegen die besten Schätzer für \\(b_j\\) am Schnittpunkt zwischen den grünen Ellipsen und der roten Linie. Durch den Verlauf der roten Linie ist die Wahrscheinlichkeit, dass sich die grünen Ellipsen und die rote Linie auf einer Koordinatenachse schneiden sehr hoch. Schneiden sich die grünen Ellipsen und die rote Linie auf einer Koordinatenachse, dann wurde ein Schätzer für einen Koeffizienten \\(b_j\\) auf Null gesetzt und somit haben wir den gewünschten Effekt der Subset Selection erreicht. 4.5 Bestimmung von \\(\\lambda\\) Der Strafterm, welcher in Gleichung () eingefügt wurde und für die Regularisierung bei LASSO verantwortlich ist, enhält eine Variable \\(\\lambda\\). Diese Variable bestimmt das Ausmass der Regularisierung und muss als zusätzlicher Parameter aus den Daten bestimmt werden. Für die Bestimmung von \\(\\lambda\\) wird eine sogenannte Kreuzvalidierungsprozedur (cross validation) verwendet. Bei einer Kreuzvalidiuerng werden die Beobachtungen zufällig in ein sogenanntes Trainings-Set und in ein Test-Set unterteilt, wobei das Test-Set meist weniger Beobachtungen enthält als das Trainings-Set. Mit dem Trainings-Set werden dann die Koeffizienten \\(\\beta_j\\) geschätzt. Dann werden für vorher bestimmte Werte von \\(\\lambda\\) die Beobachtungen im Test-Set vorhergesagt. Der Wert von \\(\\lambda\\), welcher die tiefsten Vorhersagefehler liefert, wird als optimaler Schätzwert von \\(\\lambda\\) betrachtet. 4.6 Analyse mit LASSO in R In diesem Abschnitt wird gezeigt, wie ein Datensatz mit LASSO in R analysiert werden kann. Wir verwenden dazu den Hitters- Datensatz aus dem Buch von James et al. (2013). Dieser Datensatz enthält als Zielgrösse das Einkommen von Baseballspielern und zu diesen Spielern noch weitere erklärende Variablen. Der Datensatz ist im R-Package ISLR integriert. Für die Analyse werden wir die Funktion glmnet() aus dem gleichnamigen R-Package verwenden. Als erstes installieren wir die beiden Packages und ignorieren alle Records, welche fehlende Daten aufweisen. if (!require(ISLR)) { install.packages(&quot;ISLR&quot;) require(ISLR) } if (!require(glmnet)){ install.packages(&quot;glmnet&quot;) require(glmnet) } ### # records mit fehlenden Daten ignorieren data(Hitters) Hitters &lt;- na.omit(Hitters) dim(Hitters) ## [1] 263 20 Da wir für die Bestimmung von \\(\\lambda\\) mit Kreuzvalidierung ein Trainings- und ein Test-Set benötigen, bestimmen wir diese durch den Zufallszahlengenerator und der Funktion sample() set.seed (1) train &lt;- sample (c(TRUE ,FALSE), nrow(Hitters), rep=TRUE) test &lt;- (! train ) Wir verwenden die Funktion glmnet() zur Modellierung mit LASSO. Für diese Funktion muss das Modell anders spezifiziert werden als für die Funktion lm(). Wir brauchen dazu die Objekte x und y. x &lt;- model.matrix (Salary ~ ., Hitters)[,-1] y &lt;- Hitters$Salary Die vorgegebenen Werte für \\(\\lambda\\) werden in der Variablen grid abgelegt. Es handelt sich um \\(100\\) Werte zwischen \\(10^10\\) und \\(10^{-2}\\). grid &lt;- 10^ seq (10,-2, length =100) The following statements fits a LASSO model. lasso.mod &lt;- glmnet (x[train ,],y[train],alpha =1, lambda = grid) plot(lasso.mod) Der Plot zeigt, wie sich der Strafterm für verschiedene Werte (durch Farben codiert) verhält. Nun wollen wir den besten Wert für \\(\\lambda\\) bestimmen. Dies wird durch Kreuzvalidierung gemacht. set.seed (1) cv.out &lt;- cv.glmnet (x[train ,],y[train],alpha =1) bestlam &lt;- cv.out$lambda.min Der Anteil an Koeffizienten, welcher durch LASSO null gesetzt wird kann mit folgenden Statements überprüft werden. out &lt;- glmnet(x, y, alpha = 1, lambda = grid) lasso.coef &lt;- predict(out, type = &quot;coefficients&quot;, s=bestlam )[1:20,] lasso.coef ## (Intercept) AtBat Hits HmRun Runs ## 8.898370e-01 -5.575622e-03 2.007078e+00 0.000000e+00 0.000000e+00 ## RBI Walks Years CAtBat CHits ## 0.000000e+00 2.268641e+00 -3.428874e-02 0.000000e+00 0.000000e+00 ## CHmRun CRuns CRBI CWalks LeagueN ## 8.315024e-03 2.102106e-01 4.211554e-01 0.000000e+00 1.695962e+01 ## DivisionW PutOuts Assists Errors NewLeagueN ## -1.143553e+02 2.343374e-01 0.000000e+00 -6.607899e-01 0.000000e+00 References "],
["bayes.html", "Kapitel 5 Bayes’sche Ansätze 5.1 Einführung 5.2 Das Lineare Modell 5.3 Gibbs Sampler", " Kapitel 5 Bayes’sche Ansätze 5.1 Einführung In der Statistik gibt es zwei verschiedene Lehrmeinungen. Es sind dies die Frequentisten und die Bayesianer. Alle bisher vorgestellten statistischen Konzepte, so zum Beispiel Least Squares, Maximum Likelihood, REML und BLUP stammen aus dem Lager der Frequentisten. Die Unterschiede zwischen Frequentisten und Bayesianern bestehen hauptsächlich in deren Verständnis von Wahrscheinlichkeiten deren Unterteilung von Modell- und Datenkomponenten deren Techniken zur Schätzung von Parametern Die folgende Tabelle gibt eine Übersicht über die Unterschiede. 5.2 Das Lineare Modell Die Bayes’sche Art der Parameterschätzung soll an einem einfachen linearen Modell gezeigt werden. Angenommen, wir betrachten das Modell \\[\\begin{equation} y_i = \\beta_0 + \\beta_1 x_{i1} + \\epsilon_i \\label{eq:BayLinMod} \\end{equation}\\] wobei \\(y_i\\) die \\(i\\)-te Beobachtung einer Zielgrösse ist, \\(\\beta_0\\) für den Achsenabschnitt steht, \\(x_1\\) eine erklärende Variable ist und \\(\\epsilon_i\\) für den Restterm steht. Für den Restterm nehmen wir an, dass deren Varianz konstant gleich \\(\\sigma^2\\) ist. 5.2.1 Bekannte und Unbekannte Unter der Annahme, dass wir für die Zielgrösse \\(y_i\\) und die erklärende Variable \\(x_1\\) keine fehlenden Daten haben, dann machen wir als Bayesianer folgende Einteilung in bekannte und unbekannte Grössen. und als bekannte Grössen 5.2.2 Vorgehen bei Parameterschätzung Bayesianer basieren Schätzungen von unbekannten Grössen auf der sogenannten a posteriori Verteiung der unbekannten Grössen gegeben die bekannten Grössen. Die a posteriori Verteilung wird mithilfe des Satzes von Bayes aufgrund der a priori Verteilung der unbekannten und aufgrund der Likelihood berechnet. Die Bezeichnungen “a priori” und “a posteriori” beziehen sich immer auf den Zeitpunkt der Beobachtung der analysierten Daten. Die jeweiligen Verteilungen quantifizieren den Informationsstand zu den Unbekannten um jeweiligen Zeitpunkt. Dieses Konzept soll anhand der folgenden Grafik verdeutlicht werden. Für unser Beispiel des einfachen linearen Modells, definieren wir zuerst den Vektor \\(\\mathbf{\\beta}\\) als \\[\\beta = \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array} \\right].\\] Die Beobachtungen \\(y_i\\) fassen wir ebenfalls zu einem Vektor \\(y\\) zusammen. Für den Moment nehmen wir an, dass \\(\\sigma^2\\) bekannt sei. Eine Bayes’sche Parameterschätzung für \\(\\beta\\) basiert dann auf der a posteriori Verteilung \\(f(\\beta | y, \\sigma^2)\\) der Unbekannten \\(\\beta\\) gegeben die Bekannten \\(y\\) und \\(\\sigma^2\\). Diese a posteriori Verteilung lässt sich mit dem Satz von Bayes, wie folgt berechnen \\[\\begin{eqnarray} f(\\beta | y, \\sigma^2) &amp; = &amp; \\frac{f(\\beta, \\sigma^2, y)}{f(y, \\sigma^2)} \\nonumber \\\\ &amp; = &amp; \\frac{f(y | \\beta, \\sigma^2)f(\\beta)f(\\sigma^2)}{f(y, \\sigma^2)} \\nonumber \\\\ &amp; \\propto &amp; f(y | \\beta, \\sigma^2)f(\\beta)f(\\sigma^2) \\label{LinModAPostProb} \\end{eqnarray}\\] In Gleichung () konnten wir die a posteriori Verteilung \\(f(\\beta | y, \\sigma^2)\\) als Produkt der a priori Verteilungen (\\(f(\\beta)\\) und \\(f(\\sigma^2)\\)) der unbekannten Grössen \\(\\beta\\) und \\(\\sigma^2\\) und der Likelihood \\(f(y | \\beta, \\sigma^2)\\) ausdrücken. Der Faktor \\(f(y, \\sigma^2)^{-1}\\) (Term im Nenner) entspricht der sogenannten Normalisierungskonstanten und ist nicht weiter von Interesse. Somit wird die a posteriori Verteilung oft als Proporzionalitätsbeziehung angegeben. Die a posteriori Verteilung \\(f(\\beta | y, \\sigma^2)\\) ist in vielen Fällen nicht explizit darstellbar. Das war lange ein Problem, welches die Anwendung von Bayes’schen Analysen sehr einschränkte. Zwei Entwicklungen haben dieses Problem beseitigt. In seinem Paper (Besag (1974)) zeigte Julian Besag, dass jede posteriori Verteilung durch eine Serie von Zufallszahlen aus den voll-bedingten Verteilungen bestimmt ist. Für unser Beispiel lauten die voll-bedingten Verteilungen: Bedingte Verteilung von \\(\\beta_0\\) gegeben alle anderen Grössen: \\(f(\\beta_0 | \\beta_1, \\sigma^2, y)\\), bedingte Verteilung von \\(\\beta_1\\) gegeben alle anderen Grössen: \\(f(\\beta_1 | \\beta_0, \\sigma^2, y)\\). Die Entwicklung von effizienten Pseudo-Zufallszahlen-Generatoren auf dem Computer. 5.3 Gibbs Sampler Die Umsetzung der beiden oben aufgelisteten Punkte führt zu einer Prozedur, welche als Gibbs Sampler bezeichnet wird. Wenden wir den Gibbs Sampler auf einfaches lineares Regressionsmodell an, dann resultiert das folgende Vorgehen bei der Analyse. Unabhängig vom verwendeten Modell läuft die Konstruktion einer Gibbs Sampling Prozedur immer in den folgenden Schritten ab. Diese Schritte können für die meisten Analysen wie ein Kochbuchrezept verwendet werden. Bestimmung der a priori Verteilungen für die unbekannten Grössen. Bestimmung der Likelihood Bestimmung der voll-bedingten Verteilungen 5.3.1 A priori Verteilungen In unserem Bespiel handelt es sich dabei um \\(f(\\beta)\\) und \\(f(\\sigma^2)\\). In den meisten Fällen, wenn man das erste Mal eine bestimmte Art von Daten analysisern soll, empfielt es sich eine sogenannte uniformative a priori Verteilung zu wählen. Eine uninformative a priori Verteilung bedeutet einfach, dass deren Dichtewert überall gleich, also eine Konstante ist. Wenden wir zum Beispiel für die Unbekannte \\(\\beta\\) eine uninformative a priori Verteilung an, dann bedeutet das, dass wir \\(f(\\beta) = c\\). Alternativ zu der uniformativen a priori Verteilung gibt es auch a priori Verteiungen für bestimmte unbekannte Grössen, welche als de-facto Standard aktzeptiert sind. Ein Bespiel dafür ist die a priori Verteilung der unbekannten Restvarianz, welche üblicherweise als Inverse-Chi-Quadrat Verteilung angenommen wird. 5.3.2 Likelihood Die Likelihood ist wie bei den Frequentisten als begingte Verteilung (\\(f(y | \\beta, \\sigma^2)\\)) der Daten \\(y\\) gegeben die Parameter (\\(\\beta\\) und \\(\\sigma^2\\)). Falls keine Daten fehlen, dann ist die Bayes’sche Likelihood und die frequentistische Likelihood gleich. 5.3.3 Vollbedingte Verteilungen Mit vollbedingten Verteilungen ist gemeint, dass für jede unbekannte Grösse die bedingte Verteilung gegeben alle anderen Grössen bestimmt wird. In unserem Bespiel des linearen Regressionsmodells haben wir zwei unbekannte Grössen \\(\\beta_0\\) und \\(\\beta_1\\). Somit haben wir auch zwei vollbedingte Verteilungen. Unter der Annahme, dass unsere Daten (\\(y\\)) einer Normalverteilung folgen, resultieren die folgenden vollbedingten Verteilungen. Aufgrund von Berechnungen, welche hier nicht gezeigt sind, können wir die oben aufgelisteten vollbedingten Verteilungen bestimmen. Die entsprechenden Verteilungen sind in der Kolonnen ganz rechts, welche mit “resultierende Verteilung” überschrieben ist, aufgelistet. Dabei steht \\(\\mathcal{N}()\\) für die Normalverteilung. Für die Erwartungswerte und Varianzen wird das Modell in Gleichung () leicht umformuliert. \\[\\begin{equation} \\mathbf{y} = \\mathbf{1}\\beta_0 + \\mathbf{x}\\beta_1 + \\mathbf{\\epsilon} \\label{eq:BayLinModReform} \\end{equation}\\] Aus dem obigen Modell bilden wir ein neues Modell, welches auf der rechten Seite der Gleichung nur von \\(\\beta_0\\) und \\(\\mathbf{\\epsilon}\\) abhängt. Da wir wissen, dass die Verteilung der Least Squares Schätzer eine Normalverteilung ist, werden wir diese für die Bestimmung der vollbedingten Verteilungen verwenden. \\[\\begin{equation} \\mathbf{w}_0 = \\mathbf{1}\\beta_0 + \\mathbf{\\epsilon} \\label{eq:BayLinModW0} \\end{equation}\\] wobei \\(\\mathbf{w}_0 = \\mathbf{y} - \\mathbf{x}\\beta_1\\). Aufgrund des Modells in Gleichung () können wir den Least Squares Schätzer für \\(\\beta_0\\) aufstellen. Dieser lautet: \\[\\begin{equation} \\hat{\\beta}_0 = (\\mathbf{1}^T\\mathbf{1})^{-1}\\mathbf{1}^T\\mathbf{w}_0 \\label{eq:Beta0LsEst} \\end{equation}\\] Die Varianz des Least Squares Schätzers für \\(\\beta_0\\) lautet: \\[\\begin{equation} var(\\hat{\\beta}_0) = (\\mathbf{1}^T\\mathbf{1})^{-1}\\sigma^2 \\label{eq:VarBeta0LsEst} \\end{equation}\\] Analog zu \\(\\beta_0\\) berechnen wir den Least Squares Schätzer für \\(\\beta_1\\) und dessen Varianz. \\[\\begin{equation} \\hat{\\beta}_1 = (\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{w}_1 \\label{eq:Beta1LsEst} \\end{equation}\\] wobei \\(\\mathbf{w}_1 = \\mathbf{y} - \\mathbf{1}\\beta_0\\) \\[\\begin{equation} var(\\hat{\\beta}_1) = (\\mathbf{x}^T\\mathbf{x})^{-1}\\sigma^2 \\label{eq:VarBeta1LsEst} \\end{equation}\\] 5.3.4 Umsetzung des Gibbs Samplers Der Gibbs Sampler wird durch wiederholtes ziehen von Zufallszahlen aus den oben angegebenen vollbedingten Verteilungen umgesetzt. Das heisst, wir setzen für alle unbekannten Grössen sinnvolle Startwerte ein. Für \\(\\beta_0\\) und \\(\\beta_1\\) wählen wir \\(0\\) als Startwert. Dann berechnen wir den Erwartungswert und die Varianz für die vollbedingte Verteilung von \\(\\beta_0\\). Aus dieser Verteilung ziehen wir einen neuen Wert für \\(\\beta_0\\). In einem zweiten Schritt berechnen wir den Erwartungswert und die Varianz für die vollbedingte Verteilung von \\(\\beta_1\\), wobei wir für \\(\\beta_0\\) schon den neuen Wert einsetzen. Aus der Verteilung für \\(\\beta_1\\) ziehen wir einen neuen Wert für \\(\\beta_1\\). Danach beginnen wir die Schritte wieder bei \\(\\beta_0\\). Diese Schrittabfolge wiederholen wir \\(10000\\) mal und speichern alle gezogenen Werte für \\(\\beta_0\\) und \\(\\beta_1\\). Die Bayes’schen Parameterschätzungen entsprechen dann den Mittelwerten der gespeicherten Werte. Der folgende R-Codeblock soll die Umsetzung des Gibbs Samplers für \\(\\beta_0\\) und \\(\\beta_1\\) als Programm zeigen. Der Einfachheit halber wurde \\(\\sigma^2\\) konstant \\(\\sigma^2=1\\) angenommen. # ### Startwerte für beta0 und beta1 beta &lt;– c(0, 0) # ### Bestimmung der Anzahl Iterationen niter &lt;– 10000 # ### Initialisierung des Vektors mit Resultaten meanBeta &lt;– c(0, 0) for (iter in 1:niter) { # Ziehung des Wertes des Achsenabschnitts beta0 w &lt;– y - X[, 2] * beta[2] x &lt;– X[, 1] xpxi &lt;– 1/(t(x) %*% x) betaHat &lt;– t(x) %*% w * xpxi # ### neue Zufallszahl fuer beta0 beta[1] &lt;– rnorm(1, betaHat, sqrt(xpxi)) # Ziehung der Steigung beta1 w &lt;– y - X[, 1] * beta[1] x &lt;– X[, 2] xpxi &lt;– 1/(t(x) %*% x) betaHat &lt;– t(x) %*% w * xpxi # ### neue Zufallszahl fuer beta1 beta[2] &lt;– rnorm(1, betaHat, sqrt(xpxi)) meanBeta &lt;– meanBeta + beta } # ### Ausgabe der Ergebnisse cat(sprintf(&quot;Achsenabschnitt = %6.3f \\n&quot;, meanBeta[1]/iter)) cat(sprintf(&quot;Steigung = %6.3f \\n&quot;, meanBeta[2]/iter)) References "],
["abkurzungen.html", "Abkürzungen", " Abkürzungen Abbreviation Meaning QTL Quatitative Trait Loci GWAS Genome Wide Association Study GLMM Generalized Linear Mixed Models i.i.d. independent, identically distributed QQ Quantil Quantil SNP Single Nucleotide Polymorphism BLUE Best Linear Unbiased Estimation BLUP Best Linear Unbiased Prediction GRM Genomic Relationship Matrix RR Ridge Regression GBLUP Genomic BLUP LASSO Least Absolute Shrinkage And Selection Operator REML Restricted oder Residual Maximum Likelihood ML Maximum Likelihood MCMC Markov Chain Monte Carlo "],
["references.html", "References", " References "]
]
